---
title: "W4D1 - Information Theory"
subtitle: "ì •ë³´ ì´ë¡ ê³¼ ì‹ ê²½ ì½”ë”©"
---

# W4D1: Information Theory

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yujinmin2/yujin_research/blob/main/notebooks/W4D1_InformationTheory.ipynb)

---

## ğŸ“‹ Overview

**í•µì‹¬ ì§ˆë¬¸**: ë‰´ëŸ°ì€ ì–¼ë§ˆë‚˜ ë§ì€ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ”ê°€?

**ì •ë³´ ì´ë¡ **ì€ ë¶ˆí™•ì‹¤ì„±ê³¼ ì •ë³´ ì „ë‹¬ì„ ìˆ˜í•™ì ìœ¼ë¡œ ì •ëŸ‰í™”í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

```{mermaid}
flowchart LR
    S[ìê·¹ S] --> N[ë‰´ëŸ°] --> R[ë°˜ì‘ R]
    
    H_S["H(S)<br/>ìê·¹ ì—”íŠ¸ë¡œí”¼"]
    H_R["H(R)<br/>ë°˜ì‘ ì—”íŠ¸ë¡œí”¼"]
    I["I(S;R)<br/>ìƒí˜¸ì •ë³´ëŸ‰"]
    
    S -.-> H_S
    R -.-> H_R
    S -.-> I
    R -.-> I
```

---

## ğŸ¯ Learning Objectives

1. **ì—”íŠ¸ë¡œí”¼**ì˜ ê°œë…ê³¼ ê³„ì‚° ë°©ë²• ì´í•´
2. **ìƒí˜¸ì •ë³´ëŸ‰**ìœ¼ë¡œ ì •ë³´ ì „ë‹¬ëŸ‰ ì¸¡ì •
3. **ì±„ë„ ìš©ëŸ‰**ì˜ ì˜ë¯¸ ì´í•´
4. ì‹ ê²½ ì‹œìŠ¤í…œì˜ **ì •ë³´ ì²˜ë¦¬ íš¨ìœ¨** ë¶„ì„

---

## 1. ì—”íŠ¸ë¡œí”¼ (Entropy)

### 1.1 Shannon ì—”íŠ¸ë¡œí”¼

**ì—”íŠ¸ë¡œí”¼**ëŠ” í™•ë¥  ë¶„í¬ì˜ ë¶ˆí™•ì‹¤ì„± ë˜ëŠ” "ë†€ë¼ì›€"ì˜ í‰ê· ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

$$H(X) = -\sum_{x} P(x) \log_2 P(x)$$

ë‹¨ìœ„: **bits**

```{mermaid}
flowchart LR
    subgraph ë‚®ì€ì—”íŠ¸ë¡œí”¼
        A[â—â—â—â—â—‹<br/>P=0.8, 0.2]
        A --> H1["H = 0.72 bits"]
    end
    
    subgraph ë†’ì€ì—”íŠ¸ë¡œí”¼
        B[â—â—â—‹â—‹<br/>P=0.5, 0.5]
        B --> H2["H = 1.0 bit"]
    end
```

### 1.2 êµ¬í˜„

```python
import numpy as np
import matplotlib.pyplot as plt

def entropy(p):
    """Shannon ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
    p = np.array(p)
    p = p[p > 0]  # 0 ì œì™¸ (log ì •ì˜)
    return -np.sum(p * np.log2(p))

# ì˜ˆì‹œ: ì´ì§„ ì—”íŠ¸ë¡œí”¼
p_range = np.linspace(0.01, 0.99, 100)
H_binary = [entropy([p, 1-p]) for p in p_range]

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.plot(p_range, H_binary, 'b-', linewidth=2)
plt.xlabel('P(X=1)')
plt.ylabel('H(X) (bits)')
plt.title('Binary Entropy Function')
plt.axhline(y=1, color='red', linestyle='--', label='Max = 1 bit')
plt.axvline(x=0.5, color='gray', linestyle='--')
plt.legend()
plt.grid(True, alpha=0.3)

# ì˜ˆì‹œë“¤
plt.subplot(1, 2, 2)
examples = [
    ([1.0], "í™•ì‹¤"),
    ([0.5, 0.5], "ë™ì „"),
    ([1/6]*6, "ì£¼ì‚¬ìœ„"),
    ([1/52]*52, "ì¹´ë“œ")
]

names = []
values = []
for probs, name in examples:
    H = entropy(probs)
    names.append(f"{name}\n({len(probs)}ê°œ)")
    values.append(H)

plt.bar(names, values, color=['green', 'blue', 'orange', 'red'], edgecolor='black')
plt.ylabel('Entropy (bits)')
plt.title('ë‹¤ì–‘í•œ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼')
for i, v in enumerate(values):
    plt.text(i, v + 0.1, f'{v:.2f}', ha='center')

plt.tight_layout()
plt.show()
```

### 1.3 ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼

$$H(Y|X) = \sum_x P(x) H(Y|X=x)$$

"Xë¥¼ ì•Œ ë•Œ Yì˜ ë‚¨ì€ ë¶ˆí™•ì‹¤ì„±"

---

## 2. ìƒí˜¸ì •ë³´ëŸ‰ (Mutual Information)

### 2.1 ì •ì˜

**ìƒí˜¸ì •ë³´ëŸ‰ I(X;Y)**ëŠ” Xë¥¼ ì•Œ ë•Œ Yì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„± ê°ì†ŒëŸ‰ì…ë‹ˆë‹¤.

$$I(X;Y) = H(Y) - H(Y|X) = H(X) - H(X|Y)$$

ë˜ëŠ”:

$$I(X;Y) = \sum_{x,y} P(x,y) \log_2 \frac{P(x,y)}{P(x)P(y)}$$

```{mermaid}
flowchart TB
    subgraph ë²¤ë‹¤ì´ì–´ê·¸ë¨
        HX["H(X)"]
        HY["H(Y)"]
        I["I(X;Y)<br/>ê³µìœ  ì •ë³´"]
    end
    
    HX --- I
    I --- HY
```

### 2.2 ì‹ ê²½ê³¼í•™ì—ì„œì˜ ì˜ë¯¸

$$I(S;R) = \text{ìê·¹ Sì™€ ë°˜ì‘ Rì´ ê³µìœ í•˜ëŠ” ì •ë³´ëŸ‰}$$

- **ë†’ì€ I(S;R)**: ë°˜ì‘ì´ ìê·¹ì„ ì˜ êµ¬ë¶„
- **ë‚®ì€ I(S;R)**: ë°˜ì‘ì´ ë…¸ì´ì¦ˆì— ë¬»í˜

### 2.3 êµ¬í˜„

```python
def mutual_information(P_joint):
    """
    ìƒí˜¸ì •ë³´ëŸ‰ ê³„ì‚°
    
    Parameters:
    -----------
    P_joint : 2D array - ê²°í•© í™•ë¥  ë¶„í¬ P(X,Y)
    
    Returns:
    --------
    I : float - ìƒí˜¸ì •ë³´ëŸ‰ (bits)
    """
    # ì£¼ë³€ ë¶„í¬
    P_x = np.sum(P_joint, axis=1)
    P_y = np.sum(P_joint, axis=0)
    
    # ìƒí˜¸ì •ë³´ëŸ‰ ê³„ì‚°
    I = 0
    for i in range(len(P_x)):
        for j in range(len(P_y)):
            if P_joint[i, j] > 0:
                I += P_joint[i, j] * np.log2(
                    P_joint[i, j] / (P_x[i] * P_y[j])
                )
    return I

# ì˜ˆì‹œ: ë‰´ëŸ°ì˜ ì •ë³´ ì „ë‹¬
def simulate_neuron_channel(noise_level=0.1):
    """
    ë…¸ì´ì¦ˆê°€ ìˆëŠ” ì‹ ê²½ ì±„ë„ ì‹œë®¬ë ˆì´ì…˜
    
    2ê°œ ìê·¹ (S=0, S=1) â†’ 2ê°œ ë°˜ì‘ (R=low, R=high)
    """
    # ê²°í•© í™•ë¥  P(S, R)
    # ë…¸ì´ì¦ˆ ì—†ìœ¼ë©´: S=0 â†’ R=low, S=1 â†’ R=high
    P_joint = np.array([
        [0.5 - noise_level/2, noise_level/2],      # S=0
        [noise_level/2, 0.5 - noise_level/2]       # S=1
    ])
    
    return P_joint

# ë…¸ì´ì¦ˆ ë ˆë²¨ì— ë”°ë¥¸ ìƒí˜¸ì •ë³´ëŸ‰
noise_levels = np.linspace(0, 0.5, 50)
MI_values = []

for noise in noise_levels:
    P_joint = simulate_neuron_channel(noise)
    MI_values.append(mutual_information(P_joint))

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.plot(noise_levels, MI_values, 'b-', linewidth=2)
plt.xlabel('Noise Level')
plt.ylabel('I(S;R) (bits)')
plt.title('Mutual Information vs Noise')
plt.axhline(y=1, color='red', linestyle='--', label='Channel Capacity')
plt.legend()
plt.grid(True, alpha=0.3)

# ê²°í•© ë¶„í¬ ì‹œê°í™”
plt.subplot(1, 2, 2)
P_low = simulate_neuron_channel(0.1)
P_high = simulate_neuron_channel(0.4)

im = plt.imshow(P_low, cmap='Blues', aspect='auto')
plt.colorbar(im, label='P(S,R)')
plt.xticks([0, 1], ['R=low', 'R=high'])
plt.yticks([0, 1], ['S=0', 'S=1'])
plt.xlabel('Response')
plt.ylabel('Stimulus')
plt.title(f'Joint Distribution\nI(S;R) = {mutual_information(P_low):.2f} bits')

plt.tight_layout()
plt.show()
```

---

## 3. ì‹ ê²½ ì½”ë“œì˜ ì •ë³´ëŸ‰

### 3.1 ìŠ¤íŒŒì´í¬ íŠ¸ë ˆì¸ì˜ ì—”íŠ¸ë¡œí”¼

ìŠ¤íŒŒì´í¬ íŠ¸ë ˆì¸ì„ ì´ì§„ ì‹œí€€ìŠ¤ë¡œ ê·¼ì‚¬:

$$H(R) = -\sum_r P(r) \log_2 P(r)$$

```python
def spike_train_entropy(spike_trains, bin_size=0.01, duration=1.0):
    """
    ìŠ¤íŒŒì´í¬ íŠ¸ë ˆì¸ì˜ ì—”íŠ¸ë¡œí”¼ ì¶”ì •
    
    Parameters:
    -----------
    spike_trains : list of arrays - ì—¬ëŸ¬ trialì˜ ìŠ¤íŒŒì´í¬ ì‹œê°„
    bin_size : float - ì‹œê°„ ë¹ˆ í¬ê¸° (ì´ˆ)
    """
    n_bins = int(duration / bin_size)
    
    # ê° trialì„ ì´ì§„ ë²¡í„°ë¡œ ë³€í™˜
    binary_patterns = []
    for spikes in spike_trains:
        pattern = np.zeros(n_bins, dtype=int)
        for spike in spikes:
            bin_idx = int(spike / bin_size)
            if 0 <= bin_idx < n_bins:
                pattern[bin_idx] = 1
        binary_patterns.append(tuple(pattern))
    
    # íŒ¨í„´ ë¹ˆë„ ê³„ì‚°
    from collections import Counter
    pattern_counts = Counter(binary_patterns)
    n_trials = len(spike_trains)
    
    # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
    H = 0
    for count in pattern_counts.values():
        p = count / n_trials
        if p > 0:
            H -= p * np.log2(p)
    
    return H

# ì‹œë®¬ë ˆì´ì…˜
np.random.seed(42)
n_trials = 100
duration = 0.1
rate = 50

spike_trains = []
for _ in range(n_trials):
    n_spikes = np.random.poisson(rate * duration)
    spikes = np.sort(np.random.uniform(0, duration, n_spikes))
    spike_trains.append(spikes)

H = spike_train_entropy(spike_trains, bin_size=0.01, duration=duration)
print(f"ìŠ¤íŒŒì´í¬ íŠ¸ë ˆì¸ ì—”íŠ¸ë¡œí”¼: {H:.2f} bits")
print(f"ìµœëŒ€ ê°€ëŠ¥ ì—”íŠ¸ë¡œí”¼ (10 bins): {10:.2f} bits")
```

### 3.2 ì •ë³´ ì „ë‹¬ë¥  (Information Rate)

$$\text{Information Rate} = \frac{I(S;R)}{T} \quad \text{(bits/s)}$$

---

## 4. ì±„ë„ ìš©ëŸ‰ (Channel Capacity)

### 4.1 ì •ì˜

**ì±„ë„ ìš©ëŸ‰**ì€ ì±„ë„ì„ í†µí•´ ì „ë‹¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì •ë³´ëŸ‰ì…ë‹ˆë‹¤.

$$C = \max_{P(X)} I(X;Y)$$

### 4.2 ì‹ ê²½ ì±„ë„ì˜ í•œê³„

| ì‹œìŠ¤í…œ | ì¶”ì • ìš©ëŸ‰ |
|--------|----------|
| ë‹¨ì¼ ë‰´ëŸ° | ~100 bits/s |
| ì‹œì‹ ê²½ | ~10 Mbits/s |
| ì „ì²´ ë‡Œ | ì œí•œì  (ì£¼ì˜ ì§‘ì¤‘) |

---

## ğŸ“ ì‹¤ìŠµ ë¬¸ì œ

### ë¬¸ì œ 1: íŠœë‹ ì»¤ë¸Œì™€ ì •ë³´ëŸ‰
ë‰´ëŸ°ì˜ íŠœë‹ í­ì´ ì •ë³´ ì „ë‹¬ëŸ‰ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ì„¸ìš”.

### ë¬¸ì œ 2: ìµœì  ì½”ë”©
ë™ì¼í•œ ì •ë³´ëŸ‰ì„ ì „ë‹¬í•˜ëŠ” ë° í•„ìš”í•œ ìµœì†Œ ë°œí™”ìœ¨ì„ ê³„ì‚°í•˜ì„¸ìš”.

### ë¬¸ì œ 3: Population ì •ë³´ëŸ‰
ë‰´ëŸ° ì§‘ë‹¨ì˜ ìƒí˜¸ì •ë³´ëŸ‰ì´ ê°œë³„ ë‰´ëŸ°ì˜ í•©ë³´ë‹¤ í°ì§€/ì‘ì€ì§€ ë¶„ì„í•˜ì„¸ìš”.

---

## ğŸ”— ê´€ë ¨ ê°œë…

- [ì—”íŠ¸ë¡œí”¼](../../concepts/entropy)
- [ìƒí˜¸ì •ë³´ëŸ‰](../../concepts/mutual-information)
- [Rate Coding](../../concepts/rate-coding)

---

## ğŸ“š ì°¸ê³  ìë£Œ

- Cover & Thomas, "Elements of Information Theory"
- Rieke et al., "Spikes" Chapter 2
- Borst & Theunissen, "Information Theory and Neural Coding"

---

## â­ï¸ Next

```{button-ref} day2-neural-coding
:color: primary

ë‹¤ìŒ: W4D2 - Neural Coding â†’
```
