---
title: "W4D1 - Information Theory Basics"
subtitle: "ì •ë³´ ì´ë¡  ê¸°ì´ˆ"
---

# W4D1: Information Theory Basics

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yujin/yujin_research/blob/main/notebooks/W4D1_InformationTheory.ipynb)

---

## ğŸ¯ Learning Objectives

1. ì—”íŠ¸ë¡œí”¼(Entropy)ì˜ ê°œë…
2. ìƒí˜¸ ì •ë³´ëŸ‰(Mutual Information)
3. ì‹ ê²½ ì‹œìŠ¤í…œì˜ ì±„ë„ ìš©ëŸ‰

---

## 1. Entropy

$$H(X) = -\sum_{i} p(x_i) \log_2 p(x_i)$$

```python
def entropy(probabilities):
    """ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (bits)"""
    p = np.array(probabilities)
    p = p[p > 0]  # 0 ì œì™¸
    return -np.sum(p * np.log2(p))

# ì˜ˆì‹œ
p_uniform = [0.25, 0.25, 0.25, 0.25]
p_skewed = [0.7, 0.1, 0.1, 0.1]

print(f"Uniform: H = {entropy(p_uniform):.2f} bits")
print(f"Skewed:  H = {entropy(p_skewed):.2f} bits")
```

---

## 2. Mutual Information

$$I(X;Y) = H(X) - H(X|Y)$$

ìê·¹(X)ê³¼ ì‹ ê²½ ë°˜ì‘(Y) ì‚¬ì´ì˜ ìƒí˜¸ ì •ë³´ëŸ‰ì€ ì‹ ê²½ ì‹œìŠ¤í…œì´ ì „ë‹¬í•˜ëŠ” ì •ë³´ëŸ‰ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

---

## â­ï¸ Next

```{button-ref} day2-neural-coding
:color: primary

ë‹¤ìŒ: W4D2 - Neural Information Coding â†’
```
