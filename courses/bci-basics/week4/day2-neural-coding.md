---
title: "W4D2 - Neural Coding"
subtitle: "ì‹ ê²½ ì½”ë”©ì˜ ì›ë¦¬ì™€ íš¨ìœ¨ì„±"
---

# W4D2: Neural Coding

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yujinmin2/yujin_research/blob/main/notebooks/W4D2_NeuralCoding.ipynb)

---

## ğŸ“‹ Overview

**í•µì‹¬ ì§ˆë¬¸**: ë‡ŒëŠ” ì–´ë–¤ ì½”ë”© ì „ëµì„ ì‚¬ìš©í•˜ë©°, ê·¸ê²ƒì€ ì–¼ë§ˆë‚˜ íš¨ìœ¨ì ì¸ê°€?

ì •ë³´ ì´ë¡  ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì‹ ê²½ ì½”ë”© ì „ëµì˜ íš¨ìœ¨ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.

```{mermaid}
flowchart TB
    subgraph ì½”ë”©ì „ëµ
        RATE[Rate Coding<br/>ë°œí™”ìœ¨]
        TEMP[Temporal Coding<br/>íƒ€ì´ë°]
        POP[Population Coding<br/>ì§‘ë‹¨]
        SPARSE[Sparse Coding<br/>í¬ì†Œ]
    end
    
    subgraph í‰ê°€
        INFO[ì •ë³´ëŸ‰]
        EFF[íš¨ìœ¨ì„±]
        ROB[ê°•ê±´ì„±]
    end
    
    RATE --> INFO
    TEMP --> INFO
    POP --> INFO
    SPARSE --> EFF
```

---

## ğŸ¯ Learning Objectives

1. **ë‹¤ì–‘í•œ ì‹ ê²½ ì½”ë”© ì „ëµ** ë¹„êµ
2. **Fisher Information**ìœ¼ë¡œ ì½”ë”© ì •ë°€ë„ ë¶„ì„
3. **íš¨ìœ¨ì  ì½”ë”© ê°€ì„¤** ì´í•´
4. **í¬ì†Œ ì½”ë”©**ì˜ ì¥ì  ë¶„ì„

---

## 1. ì½”ë”© ì „ëµ ë¹„êµ

### 1.1 Rate vs Temporal Coding

```python
import numpy as np
import matplotlib.pyplot as plt

def compare_coding_strategies():
    """Rate coding vs Temporal coding ì •ë³´ëŸ‰ ë¹„êµ"""
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # ì‹œë®¬ë ˆì´ì…˜ íŒŒë¼ë¯¸í„°
    duration = 0.5  # 500ms
    
    # === Rate Coding ===
    # ë°œí™”ìœ¨ì´ ìê·¹ ê°•ë„ë¥¼ ì¸ì½”ë”©
    rates = [10, 30, 50, 70]  # Hz
    
    np.random.seed(42)
    for i, rate in enumerate(rates):
        n_spikes = np.random.poisson(rate * duration)
        spike_times = np.sort(np.random.uniform(0, duration, n_spikes))
        axes[0, 0].eventplot(spike_times, lineoffsets=i, linewidths=1.5)
    
    axes[0, 0].set_yticks(range(4))
    axes[0, 0].set_yticklabels([f'{r} Hz' for r in rates])
    axes[0, 0].set_xlabel('Time (s)')
    axes[0, 0].set_title('Rate Coding\në°œí™”ìœ¨ = ìê·¹ ê°•ë„')
    
    # Rate coding ì •ë³´ëŸ‰
    # ë°œí™”ìœ¨ì´ 4ê°œ ë ˆë²¨ â†’ log2(4) = 2 bits
    axes[0, 1].bar(['4 levels', '8 levels', '16 levels'], 
                   [2, 3, 4], color='steelblue', edgecolor='black')
    axes[0, 1].set_ylabel('Information (bits)')
    axes[0, 1].set_title('Rate Coding ì •ë³´ëŸ‰\nI = log2(levels)')
    
    # === Temporal Coding ===
    # ê°™ì€ ë°œí™”ìœ¨, ë‹¤ë¥¸ íŒ¨í„´
    rate = 40
    patterns = [
        np.array([0.05, 0.06, 0.07, 0.3, 0.31, 0.32]),  # ë²„ìŠ¤íŠ¸
        np.linspace(0.05, 0.45, 6),                       # ê·œì¹™ì 
        np.array([0.1, 0.15, 0.25, 0.35, 0.4, 0.45]),    # ë¶ˆê·œì¹™
    ]
    labels = ['Burst', 'Regular', 'Irregular']
    
    for i, (pattern, label) in enumerate(zip(patterns, labels)):
        axes[1, 0].eventplot(pattern, lineoffsets=i, linewidths=1.5)
    
    axes[1, 0].set_yticks(range(3))
    axes[1, 0].set_yticklabels(labels)
    axes[1, 0].set_xlabel('Time (s)')
    axes[1, 0].set_title('Temporal Coding\nê°™ì€ ë°œí™”ìœ¨, ë‹¤ë¥¸ ì •ë³´')
    
    # Temporal coding ì •ë³´ëŸ‰
    # ì‹œê°„ í•´ìƒë„ì— ë”°ë¼ ê¸‰ê²©íˆ ì¦ê°€
    dt_values = [100, 50, 10, 5, 1]  # ms
    info_values = []
    for dt in dt_values:
        n_bins = int(500 / dt)  # 500ms ë™ì•ˆ
        max_info = n_bins  # ê° ë¹ˆì´ 0 ë˜ëŠ” 1
        info_values.append(min(max_info, 20))  # ì œí•œ
    
    axes[1, 1].plot(dt_values, info_values, 'ro-', markersize=10)
    axes[1, 1].set_xlabel('Time Resolution (ms)')
    axes[1, 1].set_ylabel('Max Information (bits)')
    axes[1, 1].set_title('Temporal Coding ì •ë³´ëŸ‰\nì‹œê°„ í•´ìƒë„ì— ë”°ë¼ ì¦ê°€')
    axes[1, 1].invert_xaxis()
    
    plt.tight_layout()
    plt.show()

compare_coding_strategies()
```

### 1.2 ì •ë³´ ì „ë‹¬ ë¹„êµ

| ì½”ë”© ì „ëµ | ì •ë³´ ìš©ëŸ‰ | ì¥ì  | ë‹¨ì  |
|----------|----------|------|------|
| **Rate** | ë‚®ìŒ (~3-4 bits) | ë…¸ì´ì¦ˆ ê°•ê±´ | ëŠë¦¼ |
| **Temporal** | ë†’ìŒ (~10+ bits) | ë¹ ë¦„, ê³ ìš©ëŸ‰ | ë…¸ì´ì¦ˆ ë¯¼ê° |
| **Population** | ë§¤ìš° ë†’ìŒ | í™•ì¥ ê°€ëŠ¥ | ì—ë„ˆì§€ ë¹„ìš© |

---

## 2. Fisher Information

### 2.1 ì •ì˜

**Fisher Information**ì€ ì¶”ì •ì˜ ì •ë°€ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.

$$J(s) = E\left[\left(\frac{\partial}{\partial s} \log P(r|s)\right)^2\right]$$

**CramÃ©r-Rao Bound**: ì–´ë–¤ ì¶”ì •ê¸°ë„ ì´ í•œê³„ë³´ë‹¤ ì¢‹ì„ ìˆ˜ ì—†ìŒ

$$\text{Var}(\hat{s}) \geq \frac{1}{J(s)}$$

### 2.2 íŠœë‹ ì»¤ë¸Œì™€ Fisher Information

íŠœë‹ ì»¤ë¸Œê°€ ê°€íŒŒë¥¼ìˆ˜ë¡ Fisher Informationì´ ë†’ìŠµë‹ˆë‹¤.

```python
def fisher_information_demo():
    """íŠœë‹ ì»¤ë¸Œ íŠ¹ì„±ê³¼ Fisher Information"""
    
    s = np.linspace(0, 180, 200)
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # ë„“ì€ íŠœë‹ vs ì¢ì€ íŠœë‹
    widths = [60, 30, 15]
    colors = ['blue', 'green', 'red']
    
    for width, color in zip(widths, colors):
        # íŠœë‹ ì»¤ë¸Œ (ê°€ìš°ì‹œì•ˆ)
        f = 50 * np.exp(-0.5 * ((s - 90) / width)**2) + 5
        axes[0, 0].plot(s, f, color=color, linewidth=2, label=f'Ïƒ={width}Â°')
        
        # íŠœë‹ ì»¤ë¸Œì˜ ê¸°ìš¸ê¸° (f')
        df = np.gradient(f, s[1] - s[0])
        axes[0, 1].plot(s, df, color=color, linewidth=2)
        
        # Fisher Information J(s) = (f')^2 / f (í¬ì•„ì†¡ ë…¸ì´ì¦ˆ ê°€ì •)
        J = df**2 / (f + 1e-10)
        axes[1, 0].plot(s, J, color=color, linewidth=2)
    
    axes[0, 0].set_xlabel('Stimulus (Â°)')
    axes[0, 0].set_ylabel('Firing Rate (Hz)')
    axes[0, 0].set_title('Tuning Curves')
    axes[0, 0].legend()
    
    axes[0, 1].set_xlabel('Stimulus (Â°)')
    axes[0, 1].set_ylabel("f'(s)")
    axes[0, 1].set_title('Tuning Curve Slope')
    axes[0, 1].axhline(y=0, color='gray', linestyle='--')
    
    axes[1, 0].set_xlabel('Stimulus (Â°)')
    axes[1, 0].set_ylabel('J(s)')
    axes[1, 0].set_title('Fisher Information\nì¢ì€ íŠœë‹ â†’ ë†’ì€ ì •ë°€ë„')
    
    # Population Fisher Information
    # ì—¬ëŸ¬ ë‰´ëŸ°ì˜ Fisher Information í•©
    n_neurons = [4, 8, 16, 32]
    total_J = []
    
    for N in n_neurons:
        preferred = np.linspace(0, 180, N)
        J_sum = np.zeros_like(s)
        for pref in preferred:
            f = 50 * np.exp(-0.5 * ((s - pref) / 30)**2) + 5
            df = np.gradient(f, s[1] - s[0])
            J_sum += df**2 / (f + 1e-10)
        total_J.append(np.mean(J_sum))
    
    axes[1, 1].plot(n_neurons, total_J, 'ko-', markersize=10, linewidth=2)
    axes[1, 1].set_xlabel('Number of Neurons')
    axes[1, 1].set_ylabel('Total Fisher Information')
    axes[1, 1].set_title('Population Size vs Information\nì„ í˜• ì¦ê°€')
    
    plt.tight_layout()
    plt.show()

fisher_information_demo()
```

---

## 3. íš¨ìœ¨ì  ì½”ë”© ê°€ì„¤ (Efficient Coding)

### 3.1 ê°œë…

**íš¨ìœ¨ì  ì½”ë”© ê°€ì„¤**: ê°ê° ì‹œìŠ¤í…œì€ ìì—° í™˜ê²½ì˜ í†µê³„ì— ì ì‘í•˜ì—¬ ì •ë³´ ì „ë‹¬ì„ ìµœëŒ€í™”í•˜ë„ë¡ ì§„í™”í–ˆë‹¤.

```{mermaid}
flowchart LR
    ENV[ìì—° í™˜ê²½<br/>í†µê³„ êµ¬ì¡°] --> ADAPT[ì‹ ê²½ ì ì‘]
    ADAPT --> OPT[ìµœì  ì½”ë”©<br/>ì •ë³´ ìµœëŒ€í™”]
    
    subgraph ì˜ˆì‹œ
        RETINA[ë§ë§‰<br/>ëª…ì•” ëŒ€ë¹„]
        V1[V1<br/>ë°©í–¥/ì—£ì§€]
        IT[IT<br/>ë¬¼ì²´]
    end
```

### 3.2 íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”

```python
def efficient_coding_demo():
    """íš¨ìœ¨ì  ì½”ë”©: ì…ë ¥ ë¶„í¬ì— ì ì‘"""
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # ìì—° ì˜ìƒì˜ ë°ê¸° ë¶„í¬ (ë¹„ê· ì¼)
    np.random.seed(42)
    natural_dist = np.concatenate([
        np.random.normal(0.3, 0.1, 3000),
        np.random.normal(0.7, 0.15, 1000)
    ])
    natural_dist = np.clip(natural_dist, 0, 1)
    
    axes[0, 0].hist(natural_dist, bins=50, density=True, alpha=0.7, 
                    color='gray', edgecolor='black')
    axes[0, 0].set_xlabel('Luminance')
    axes[0, 0].set_ylabel('Probability')
    axes[0, 0].set_title('Natural Image Statistics\n(ë¹„ê· ì¼ ë¶„í¬)')
    
    # ì„ í˜• ì¸ì½”ë”© (ë¹„íš¨ìœ¨ì )
    def linear_encoding(x):
        return x
    
    linear_response = linear_encoding(natural_dist)
    axes[0, 1].hist(linear_response, bins=50, density=True, alpha=0.7,
                    color='blue', edgecolor='black')
    axes[0, 1].set_xlabel('Neural Response')
    axes[0, 1].set_ylabel('Probability')
    axes[0, 1].set_title('Linear Encoding\n(ì •ë³´ ì†ì‹¤)')
    
    # íš¨ìœ¨ì  ì¸ì½”ë”© (íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”)
    from scipy.stats import rankdata
    def efficient_encoding(x):
        return rankdata(x) / len(x)
    
    efficient_response = efficient_encoding(natural_dist)
    axes[1, 0].hist(efficient_response, bins=50, density=True, alpha=0.7,
                    color='green', edgecolor='black')
    axes[1, 0].set_xlabel('Neural Response')
    axes[1, 0].set_ylabel('Probability')
    axes[1, 0].set_title('Efficient Encoding\n(ê· ì¼ ë¶„í¬ â†’ ìµœëŒ€ ì—”íŠ¸ë¡œí”¼)')
    
    # ì¸ì½”ë”© í•¨ìˆ˜ ë¹„êµ
    x_sorted = np.sort(natural_dist)
    axes[1, 1].plot(x_sorted, linear_encoding(x_sorted), 'b-', 
                    linewidth=2, label='Linear')
    axes[1, 1].plot(x_sorted, np.linspace(0, 1, len(x_sorted)), 'g-',
                    linewidth=2, label='Efficient (CDF)')
    axes[1, 1].set_xlabel('Input (Luminance)')
    axes[1, 1].set_ylabel('Output (Response)')
    axes[1, 1].set_title('Encoding Functions')
    axes[1, 1].legend()
    
    plt.tight_layout()
    plt.show()
    
    # ì •ë³´ëŸ‰ ë¹„êµ
    def entropy_from_hist(data, bins=50):
        hist, _ = np.histogram(data, bins=bins, density=True)
        hist = hist[hist > 0]
        hist = hist / hist.sum()
        return -np.sum(hist * np.log2(hist + 1e-10))
    
    H_linear = entropy_from_hist(linear_response)
    H_efficient = entropy_from_hist(efficient_response)
    
    print(f"Linear encoding entropy: {H_linear:.2f} bits")
    print(f"Efficient encoding entropy: {H_efficient:.2f} bits")
    print(f"Max possible (uniform): {np.log2(50):.2f} bits")

efficient_coding_demo()
```

---

## 4. í¬ì†Œ ì½”ë”© (Sparse Coding)

### 4.1 ê°œë…

**í¬ì†Œ ì½”ë”©**: ì£¼ì–´ì§„ ì‹œê°„ì— ì†Œìˆ˜ì˜ ë‰´ëŸ°ë§Œ í™œì„±í™”ë˜ëŠ” í‘œí˜„ ë°©ì‹

| íŠ¹ì„± | ë°€ì§‘ ì½”ë”© | í¬ì†Œ ì½”ë”© |
|------|----------|----------|
| í™œì„± ë‰´ëŸ° | ë‹¤ìˆ˜ | ì†Œìˆ˜ |
| ì—ë„ˆì§€ íš¨ìœ¨ | ë‚®ìŒ | ë†’ìŒ |
| í‘œí˜„ ìš©ëŸ‰ | ë‚®ìŒ | ë†’ìŒ |
| ì˜ˆì‹œ | ì´ˆê¸° ê°ê° | í•´ë§ˆ, IT í”¼ì§ˆ |

### 4.2 í‘œí˜„ ìš©ëŸ‰

```python
def sparse_coding_capacity():
    """í¬ì†Œ ì½”ë”©ì˜ í‘œí˜„ ìš©ëŸ‰"""
    
    N = 1000  # ì´ ë‰´ëŸ° ìˆ˜
    K_values = range(1, 101)  # í™œì„± ë‰´ëŸ° ìˆ˜
    
    # ì¡°í•©ì˜ ìˆ˜: C(N, K)
    from scipy.special import comb
    capacities = [comb(N, K, exact=False) for K in K_values]
    
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.semilogy(K_values, capacities, 'b-', linewidth=2)
    plt.xlabel('Active Neurons (K)')
    plt.ylabel('Number of Patterns')
    plt.title(f'Sparse Coding Capacity (N={N})')
    plt.axvline(x=N//2, color='red', linestyle='--', label=f'K=N/2 (max)')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    sparsity = np.array(K_values) / N
    info_bits = np.log2(np.array(capacities) + 1)
    plt.plot(sparsity * 100, info_bits, 'g-', linewidth=2)
    plt.xlabel('Sparsity (%)')
    plt.ylabel('Information (bits)')
    plt.title('Information vs Sparsity')
    
    plt.tight_layout()
    plt.show()
    
    # ìµœì  í¬ì†Œì„±
    optimal_K = K_values[np.argmax(capacities)]
    print(f"ìµœì  í™œì„± ë‰´ëŸ° ìˆ˜: {optimal_K} / {N}")
    print(f"ìµœì  í¬ì†Œì„±: {optimal_K/N*100:.1f}%")

sparse_coding_capacity()
```

---

## ğŸ“ ì‹¤ìŠµ ë¬¸ì œ

### ë¬¸ì œ 1: ìµœì  íŠœë‹ í­
ì£¼ì–´ì§„ ìê·¹ ë²”ìœ„ì—ì„œ Fisher Informationì„ ìµœëŒ€í™”í•˜ëŠ” íŠœë‹ í­ì„ ì°¾ìœ¼ì„¸ìš”.

### ë¬¸ì œ 2: íš¨ìœ¨ì  ì½”ë”©
ìì—° ì˜ìƒ ë°ì´í„°ì…‹ì—ì„œ ìµœì ì˜ ì¸ì½”ë”© í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ì„¸ìš”.

### ë¬¸ì œ 3: í¬ì†Œì„±ê³¼ ë…¸ì´ì¦ˆ
í¬ì†Œ ì½”ë”©ì´ ë…¸ì´ì¦ˆì— ëŒ€í•œ ê°•ê±´ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ì„¸ìš”.

---

## ğŸ”— ê´€ë ¨ ê°œë…

- [ì—”íŠ¸ë¡œí”¼](../../concepts/entropy)
- [ìƒí˜¸ì •ë³´ëŸ‰](../../concepts/mutual-information)
- [íŠœë‹ ì»¤ë¸Œ](../../concepts/tuning-curve)
- [Fisher Information](../../concepts/fisher-information)

---

## ğŸ“š ì°¸ê³  ìë£Œ

- Simoncelli & Olshausen, "Natural Image Statistics and Neural Representation"
- Barlow, "Possible Principles Underlying the Transformation of Sensory Messages"
- Olshausen & Field, "Sparse Coding of Sensory Inputs"

---

## â­ï¸ Next

```{button-ref} ../week5/day1-hodgkin-huxley
:color: primary

ë‹¤ìŒ: W5D1 - Hodgkin-Huxley Model â†’
```
