---
title: "W3D1 - Neural Decoding"
subtitle: "ì‹ ê²½ ë””ì½”ë”©: ë‡Œ ì‹ í˜¸ì—ì„œ ì •ë³´ ì¶”ì¶œí•˜ê¸°"
---

# W3D1: Neural Decoding

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yujinmin2/yujin_research/blob/main/notebooks/W3D1_NeuralDecoding.ipynb)

---

## ğŸ“‹ Overview

**í•µì‹¬ ì§ˆë¬¸**: ì‹ ê²½ í™œë™ì„ ê´€ì°°í•˜ì—¬ ìê·¹ì´ë‚˜ í–‰ë™ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”ê°€?

**ì‹ ê²½ ë””ì½”ë”©**ì€ ì¸ì½”ë”©ì˜ ì—­ê³¼ì •ìœ¼ë¡œ, ì‹ ê²½ ë°˜ì‘ìœ¼ë¡œë¶€í„° ìê·¹ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

```{mermaid}
flowchart LR
    subgraph ì¸ì½”ë”©
        S1[ìê·¹ s] --> R1[ë°˜ì‘ r]
    end
    
    subgraph ë””ì½”ë”©
        R2[ë°˜ì‘ r] --> S2[ì¶”ì • Å]
    end
    
    style S1 fill:#3498db
    style R1 fill:#e74c3c
    style R2 fill:#e74c3c
    style S2 fill:#2ecc71
```

---

## ğŸ¯ Learning Objectives

1. **ë””ì½”ë”© ë¬¸ì œ**ì˜ ì •ì˜ì™€ ì¤‘ìš”ì„± ì´í•´
2. **ë² ì´ì§€ì•ˆ ë””ì½”ë”©** í”„ë ˆì„ì›Œí¬ ì´í•´
3. **Maximum Likelihood** ì¶”ì • êµ¬í˜„
4. **Population Vector** ë””ì½”ë”© êµ¬í˜„
5. ë‹¤ì–‘í•œ **ë””ì½”ë” ì„±ëŠ¥ í‰ê°€** ë°©ë²• ì´í•´

---

## ğŸ“š ë°°ê²½: ì™œ ë””ì½”ë”©ì¸ê°€?

### ê³¼í•™ì  ëª©í‘œ
- ë‡Œê°€ ì–´ë–¤ ì •ë³´ë¥¼ í‘œí˜„í•˜ëŠ”ì§€ ê²€ì¦
- ì‹ ê²½ ì½”ë“œì˜ íŠ¹ì„± ì´í•´

### ê³µí•™ì  ëª©í‘œ
- **BCI**: ë‡Œ ì‹ í˜¸ë¡œ ê¸°ê¸° ì œì–´
- **ì‹ ê²½ ë³´ì² **: ê°ê° ì •ë³´ ë³µì›

---

## 1. ë² ì´ì§€ì•ˆ ë””ì½”ë”© (Bayesian Decoding)

### 1.1 ë² ì´ì¦ˆ ì •ë¦¬

> ğŸ’¡ **í•µì‹¬ ì•„ì´ë””ì–´**: ê´€ì¸¡ëœ ì‹ ê²½ ë°˜ì‘ $r$ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ìê·¹ $s$ì˜ í™•ë¥  ë¶„í¬ë¥¼ ê³„ì‚°

$$P(s|r) = \frac{P(r|s) \cdot P(s)}{P(r)}$$

```{mermaid}
flowchart TB
    subgraph ì…ë ¥
        R[ì‹ ê²½ ë°˜ì‘ r]
        PRIOR[ì‚¬ì „í™•ë¥  P/s/]
    end
    
    subgraph ëª¨ë¸
        L[ìš°ë„ P/r|s/<br/>íŠœë‹ ì»¤ë¸Œì—ì„œ ìœ ë„]
    end
    
    subgraph ì¶œë ¥
        POST[ì‚¬í›„í™•ë¥  P/s|r/]
        EST[ì¶”ì •ê°’ Å]
    end
    
    R --> L
    L --> POST
    PRIOR --> POST
    POST --> EST
```

### 1.2 êµ¬ì„± ìš”ì†Œ

| ìš”ì†Œ | ìˆ˜ì‹ | ì˜ë¯¸ | ì¶œì²˜ |
|------|------|------|------|
| **ìš°ë„ (Likelihood)** | $P(r\|s)$ | ìê·¹ì´ sì¼ ë•Œ ë°˜ì‘ rì˜ í™•ë¥  | íŠœë‹ ì»¤ë¸Œ |
| **ì‚¬ì „í™•ë¥  (Prior)** | $P(s)$ | ìê·¹ì˜ ê¸°ë³¸ ë¶„í¬ | ê²½í—˜/ê°€ì • |
| **ì‚¬í›„í™•ë¥  (Posterior)** | $P(s\|r)$ | ë°˜ì‘ì´ rì¼ ë•Œ ìê·¹ì˜ í™•ë¥  | ê³„ì‚° ê²°ê³¼ |

### 1.3 í¬ì•„ì†¡ ìš°ë„ í•¨ìˆ˜

ë‰´ëŸ°ì˜ ë°œí™”ê°€ í¬ì•„ì†¡ ê³¼ì •ì„ ë”°ë¥¸ë‹¤ë©´:

$$P(r|s) = \prod_{i=1}^{N} \frac{f_i(s)^{r_i}}{r_i!} e^{-f_i(s)}$$

ì—¬ê¸°ì„œ $f_i(s)$ëŠ” ë‰´ëŸ° $i$ì˜ íŠœë‹ ì»¤ë¸Œ

```python
import numpy as np
from scipy.stats import poisson

def poisson_log_likelihood(response, tuning_curves, stimuli):
    """
    í¬ì•„ì†¡ ë¡œê·¸ ìš°ë„ ê³„ì‚°
    
    Parameters:
    -----------
    response : array (N,) - ê° ë‰´ëŸ°ì˜ ìŠ¤íŒŒì´í¬ ìˆ˜
    tuning_curves : array (N, S) - ê° ë‰´ëŸ°ì˜ íŠœë‹ ì»¤ë¸Œ
    stimuli : array (S,) - ê°€ëŠ¥í•œ ìê·¹ ê°’ë“¤
    
    Returns:
    --------
    log_likelihood : array (S,) - ê° ìê·¹ì— ëŒ€í•œ ë¡œê·¸ ìš°ë„
    """
    N_neurons = len(response)
    N_stimuli = len(stimuli)
    
    log_likelihood = np.zeros(N_stimuli)
    
    for s_idx in range(N_stimuli):
        for n in range(N_neurons):
            # ì˜ˆìƒ ë°œí™”ìœ¨
            expected_rate = tuning_curves[n, s_idx]
            # ì‹¤ì œ ë°˜ì‘
            observed = response[n]
            # í¬ì•„ì†¡ ë¡œê·¸ í™•ë¥ 
            log_likelihood[s_idx] += poisson.logpmf(observed, expected_rate + 1e-10)
    
    return log_likelihood

# ì˜ˆì‹œ
np.random.seed(42)
N_neurons = 8
N_stimuli = 180

# íŠœë‹ ì»¤ë¸Œ ìƒì„± (ë°©í–¥ ì„ íƒì„±)
stimuli = np.linspace(0, 180, N_stimuli)
preferred_dirs = np.linspace(0, 160, N_neurons)
tuning_curves = np.zeros((N_neurons, N_stimuli))

for n, pref in enumerate(preferred_dirs):
    tuning_curves[n] = 30 * np.exp(-0.5 * ((stimuli - pref) / 30)**2) + 5

# ìê·¹ = 60ë„ì¼ ë•Œì˜ ë°˜ì‘ ì‹œë®¬ë ˆì´ì…˜
true_stimulus = 60
true_idx = np.argmin(np.abs(stimuli - true_stimulus))
response = np.random.poisson(tuning_curves[:, true_idx])

# ë””ì½”ë”©
log_like = poisson_log_likelihood(response, tuning_curves, stimuli)
decoded_idx = np.argmax(log_like)
decoded_stimulus = stimuli[decoded_idx]

print(f"ì‹¤ì œ ìê·¹: {true_stimulus}Â°")
print(f"ë””ì½”ë”©ëœ ìê·¹: {decoded_stimulus:.1f}Â°")
```

---

## 2. Maximum Likelihood Estimation (MLE)

### 2.1 ê°œë…

**MLE**ëŠ” ê´€ì¸¡ëœ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

$$\hat{s}_{ML} = \arg\max_s P(r|s)$$

> ğŸ“Œ ì‚¬ì „í™•ë¥ ì´ ê· ì¼(uniform)í•˜ë©´ MAP = MLE

### 2.2 ì‹œê°í™”

```python
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1. íŠœë‹ ì»¤ë¸Œ
for n in range(N_neurons):
    axes[0, 0].plot(stimuli, tuning_curves[n], label=f'N{n+1}: {preferred_dirs[n]:.0f}Â°')
axes[0, 0].axvline(x=true_stimulus, color='red', linestyle='--', linewidth=2, label='True stimulus')
axes[0, 0].set_xlabel('Stimulus Direction (Â°)')
axes[0, 0].set_ylabel('Firing Rate (Hz)')
axes[0, 0].set_title('Tuning Curves')
axes[0, 0].legend(fontsize=8)

# 2. ê´€ì¸¡ëœ ë°˜ì‘
axes[0, 1].bar(range(N_neurons), response, color='steelblue', edgecolor='black')
axes[0, 1].set_xlabel('Neuron')
axes[0, 1].set_ylabel('Spike Count')
axes[0, 1].set_title(f'Observed Response (True: {true_stimulus}Â°)')
axes[0, 1].set_xticks(range(N_neurons))
axes[0, 1].set_xticklabels([f'{p:.0f}Â°' for p in preferred_dirs], rotation=45)

# 3. ë¡œê·¸ ìš°ë„
axes[1, 0].plot(stimuli, log_like, 'b-', linewidth=2)
axes[1, 0].axvline(x=true_stimulus, color='red', linestyle='--', label='True')
axes[1, 0].axvline(x=decoded_stimulus, color='green', linestyle='--', label='Decoded')
axes[1, 0].set_xlabel('Stimulus Direction (Â°)')
axes[1, 0].set_ylabel('Log Likelihood')
axes[1, 0].set_title('Maximum Likelihood Decoding')
axes[1, 0].legend()

# 4. ì‚¬í›„í™•ë¥  (ì •ê·œí™”)
posterior = np.exp(log_like - np.max(log_like))
posterior = posterior / np.sum(posterior)
axes[1, 1].fill_between(stimuli, posterior, alpha=0.5, color='purple')
axes[1, 1].plot(stimuli, posterior, 'purple', linewidth=2)
axes[1, 1].axvline(x=true_stimulus, color='red', linestyle='--', label='True')
axes[1, 1].axvline(x=decoded_stimulus, color='green', linestyle='--', label='MAP')
axes[1, 1].set_xlabel('Stimulus Direction (Â°)')
axes[1, 1].set_ylabel('P(s|r)')
axes[1, 1].set_title('Posterior Distribution')
axes[1, 1].legend()

plt.tight_layout()
plt.show()
```

---

## 3. Population Vector Decoding

### 3.1 ê°œë…

**Population Vector**ëŠ” ê° ë‰´ëŸ°ì˜ ì„ í˜¸ ë°©í–¥ì„ ë°œí™”ìœ¨ë¡œ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìê·¹ì„ ì¶”ì •í•©ë‹ˆë‹¤.

$$\vec{P} = \sum_{i=1}^{N} r_i \cdot \vec{c}_i$$

ì—¬ê¸°ì„œ:
- $r_i$: ë‰´ëŸ° $i$ì˜ ë°œí™”ìœ¨
- $\vec{c}_i$: ë‰´ëŸ° $i$ì˜ ì„ í˜¸ ë°©í–¥ ë‹¨ìœ„ ë²¡í„°

```{mermaid}
flowchart TB
    subgraph ë‰´ëŸ°í™œë™
        N1[N1: ì„ í˜¸ 0Â°<br/>r=10] 
        N2[N2: ì„ í˜¸ 45Â°<br/>r=35]
        N3[N3: ì„ í˜¸ 90Â°<br/>r=25]
        N4[N4: ì„ í˜¸ 135Â°<br/>r=8]
    end
    
    PV[Population Vector<br/>ë²¡í„° í•©ì‚°]
    
    N1 --> PV
    N2 --> PV
    N3 --> PV
    N4 --> PV
    
    PV --> EST[ì¶”ì • ë°©í–¥: ~55Â°]
```

### 3.2 êµ¬í˜„

```python
def population_vector_decode(response, preferred_directions):
    """
    Population Vector ë””ì½”ë”©
    
    Parameters:
    -----------
    response : array (N,) - ê° ë‰´ëŸ°ì˜ ë°œí™”ìœ¨
    preferred_directions : array (N,) - ê° ë‰´ëŸ°ì˜ ì„ í˜¸ ë°©í–¥ (ë„)
    
    Returns:
    --------
    decoded_direction : float - ë””ì½”ë”©ëœ ë°©í–¥ (ë„)
    """
    # ì„ í˜¸ ë°©í–¥ì„ ë¼ë””ì•ˆìœ¼ë¡œ ë³€í™˜
    pref_rad = np.deg2rad(preferred_directions)
    
    # ê° ë‰´ëŸ°ì˜ ê¸°ì—¬ë¥¼ ë²¡í„°ë¡œ
    x = np.sum(response * np.cos(pref_rad))
    y = np.sum(response * np.sin(pref_rad))
    
    # ë°©í–¥ ê³„ì‚°
    decoded_rad = np.arctan2(y, x)
    decoded_deg = np.rad2deg(decoded_rad)
    
    # 0-180 ë²”ìœ„ë¡œ
    if decoded_deg < 0:
        decoded_deg += 180
    
    return decoded_deg

# í…ŒìŠ¤íŠ¸
pv_decoded = population_vector_decode(response, preferred_dirs)
print(f"Population Vector ë””ì½”ë”©: {pv_decoded:.1f}Â°")
print(f"ì‹¤ì œ ìê·¹: {true_stimulus}Â°")
print(f"ì˜¤ì°¨: {abs(pv_decoded - true_stimulus):.1f}Â°")
```

### 3.3 ì‹œê°í™” (ê·¹ì¢Œí‘œ)

```python
fig = plt.figure(figsize=(10, 5))

# ê·¹ì¢Œí‘œ í”Œë¡¯
ax = fig.add_subplot(121, projection='polar')

# ê° ë‰´ëŸ°ì˜ ê¸°ì—¬
colors = plt.cm.viridis(np.linspace(0, 1, N_neurons))
for n, (pref, r) in enumerate(zip(preferred_dirs, response)):
    ax.arrow(np.deg2rad(pref), 0, 0, r/np.max(response),
             head_width=0.1, head_length=0.05,
             fc=colors[n], ec='black', linewidth=0.5, alpha=0.7)

# Population Vector
ax.arrow(0, 0, np.deg2rad(pv_decoded), 0.8,
         head_width=0.15, head_length=0.08,
         fc='red', ec='darkred', linewidth=2)

# ì‹¤ì œ ë°©í–¥
ax.plot([0, np.deg2rad(true_stimulus)], [0, 1], 'g--', linewidth=2)

ax.set_title('Population Vector Decoding')

# ì˜¤ì°¨ ë¶„í¬ (ì—¬ëŸ¬ trial ì‹œë®¬ë ˆì´ì…˜)
ax2 = fig.add_subplot(122)

errors = []
for _ in range(500):
    # ë°˜ì‘ ì‹œë®¬ë ˆì´ì…˜
    sim_response = np.random.poisson(tuning_curves[:, true_idx])
    # ë””ì½”ë”©
    decoded = population_vector_decode(sim_response, preferred_dirs)
    errors.append(decoded - true_stimulus)

ax2.hist(errors, bins=30, edgecolor='black', alpha=0.7)
ax2.axvline(x=0, color='red', linestyle='--')
ax2.set_xlabel('Decoding Error (Â°)')
ax2.set_ylabel('Count')
ax2.set_title(f'Error Distribution\nMean: {np.mean(errors):.1f}Â°, Std: {np.std(errors):.1f}Â°')

plt.tight_layout()
plt.show()
```

---

## 4. ë””ì½”ë” ì„±ëŠ¥ í‰ê°€

### 4.1 í‰ê°€ ì§€í‘œ

| ì§€í‘œ | ìˆ˜ì‹ | ì˜ë¯¸ |
|------|------|------|
| **MSE** | $\frac{1}{N}\sum(\hat{s}-s)^2$ | í‰ê·  ì œê³± ì˜¤ì°¨ |
| **Bias** | $E[\hat{s}] - s$ | í¸í–¥ |
| **Variance** | $Var[\hat{s}]$ | ë¶„ì‚° |
| **ì •ë³´ëŸ‰** | $I(S;\hat{S})$ | ìƒí˜¸ì •ë³´ëŸ‰ |

### 4.2 ë””ì½”ë” ë¹„êµ

```python
def compare_decoders(n_trials=100):
    """MLE vs Population Vector ë¹„êµ"""
    
    mle_errors = []
    pv_errors = []
    
    test_stimuli = np.linspace(20, 160, 8)
    
    for true_stim in test_stimuli:
        true_idx = np.argmin(np.abs(stimuli - true_stim))
        
        for _ in range(n_trials):
            # ë°˜ì‘ ì‹œë®¬ë ˆì´ì…˜
            resp = np.random.poisson(tuning_curves[:, true_idx])
            
            # MLE ë””ì½”ë”©
            log_like = poisson_log_likelihood(resp, tuning_curves, stimuli)
            mle_decoded = stimuli[np.argmax(log_like)]
            mle_errors.append(mle_decoded - true_stim)
            
            # PV ë””ì½”ë”©
            pv_decoded = population_vector_decode(resp, preferred_dirs)
            pv_errors.append(pv_decoded - true_stim)
    
    return np.array(mle_errors), np.array(pv_errors)

mle_err, pv_err = compare_decoders()

print("=== ë””ì½”ë” ì„±ëŠ¥ ë¹„êµ ===")
print(f"MLE - Bias: {np.mean(mle_err):.2f}Â°, RMSE: {np.sqrt(np.mean(mle_err**2)):.2f}Â°")
print(f"PV  - Bias: {np.mean(pv_err):.2f}Â°, RMSE: {np.sqrt(np.mean(pv_err**2)):.2f}Â°")
```

---

## ğŸ“ ì‹¤ìŠµ ë¬¸ì œ

### ë¬¸ì œ 1: ë² ì´ì§€ì•ˆ ë””ì½”ë”©
ë¹„ê· ì¼ ì‚¬ì „í™•ë¥ ì„ ì ìš©í•œ MAP ë””ì½”ë”ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.

### ë¬¸ì œ 2: ë””ì½”ë” íŠœë‹
ë‰´ëŸ° ìˆ˜(N=4, 8, 16, 32)ì— ë”°ë¥¸ ë””ì½”ë”© ì •í™•ë„ë¥¼ ë¹„êµí•˜ì„¸ìš”.

### ë¬¸ì œ 3: ì‹¤ì œ ë°ì´í„°
Allen Brain Observatory ë°ì´í„°ì— ë””ì½”ë”ë¥¼ ì ìš©í•´ë³´ì„¸ìš”.

---

## ğŸ”— ê´€ë ¨ ê°œë…

- [ë² ì´ì§€ì•ˆ ë””ì½”ë”©](../../concepts/bayesian-decoding)
- [íŠœë‹ ì»¤ë¸Œ](../../concepts/tuning-curve)
- [Population Vector](../../concepts/population-vector)

---

## ğŸ“š ì°¸ê³  ìë£Œ

- Dayan & Abbott, Chapter 3: Neural Decoding
- Pouget et al. (2000): Information Processing with Population Codes
- Neuromatch Academy: Decoding Models

---

## â­ï¸ Next

```{button-ref} day2-bci-applications
:color: primary

ë‹¤ìŒ: W3D2 - BCI Applications â†’
```
