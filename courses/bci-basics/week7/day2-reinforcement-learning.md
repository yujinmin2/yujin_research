---
title: "W7D2 - Reinforcement Learning"
subtitle: "ê°•í™” í•™ìŠµê³¼ ë³´ìƒ ê¸°ë°˜ í•™ìŠµ"
---

# W7D2: Reinforcement Learning

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yujinmin2/yujin_research/blob/main/notebooks/W7D2_ReinforcementLearning.ipynb)

---

## ğŸ“‹ Overview

**í•µì‹¬ ì§ˆë¬¸**: ë‡ŒëŠ” ì–´ë–»ê²Œ ë³´ìƒ ì‹ í˜¸ë¡œë¶€í„° í•™ìŠµí•˜ëŠ”ê°€?

**ê°•í™” í•™ìŠµ**ì€ ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” í–‰ë™ì„ ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•´ í•™ìŠµí•©ë‹ˆë‹¤.

```{mermaid}
flowchart LR
    A[ì—ì´ì „íŠ¸<br/>Agent] --> |í–‰ë™ a| E[í™˜ê²½<br/>Environment]
    E --> |ìƒíƒœ s| A
    E --> |ë³´ìƒ r| A
    
    style A fill:#3498db
    style E fill:#2ecc71
```

---

## ğŸ¯ Learning Objectives

1. **ê°•í™” í•™ìŠµ í”„ë ˆì„ì›Œí¬** ì´í•´
2. **TD í•™ìŠµ**ê³¼ ë„íŒŒë¯¼ ì‹ í˜¸ì˜ ì—°ê²°
3. **Q-ëŸ¬ë‹** êµ¬í˜„
4. **Actor-Critic** ëª¨ë¸ ì´í•´

---

## 1. ê°•í™” í•™ìŠµ ê¸°ì´ˆ

### 1.1 í•µì‹¬ ê°œë…

| ê°œë… | ê¸°í˜¸ | ì„¤ëª… |
|------|------|------|
| **ìƒíƒœ** | $s$ | í™˜ê²½ì˜ í˜„ì¬ ìƒí™© |
| **í–‰ë™** | $a$ | ì—ì´ì „íŠ¸ì˜ ì„ íƒ |
| **ë³´ìƒ** | $r$ | ì¦‰ê°ì  í”¼ë“œë°± |
| **ì •ì±…** | $\pi(a|s)$ | í–‰ë™ ì„ íƒ ê·œì¹™ |
| **ê°€ì¹˜ í•¨ìˆ˜** | $V(s)$ | ìƒíƒœì˜ ê¸°ëŒ€ ë³´ìƒ |
| **í–‰ë™ ê°€ì¹˜** | $Q(s,a)$ | í–‰ë™ì˜ ê¸°ëŒ€ ë³´ìƒ |

### 1.2 ëª©í‘œ

$$\max_\pi \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t\right]$$

$\gamma$: í• ì¸ ê³„ìˆ˜ (ë¯¸ë˜ ë³´ìƒì˜ ê°€ì¹˜)

---

## 2. TD í•™ìŠµê³¼ ë„íŒŒë¯¼

### 2.1 Temporal Difference (TD) í•™ìŠµ

$$V(s_t) \leftarrow V(s_t) + \alpha \cdot \underbrace{[r_t + \gamma V(s_{t+1}) - V(s_t)]}_{\text{TD Error } \delta}$$

### 2.2 ë„íŒŒë¯¼ = TD Error?

```{mermaid}
flowchart TB
    subgraph ì‹ ê²½ê³¼í•™
        DA[ë„íŒŒë¯¼ ë‰´ëŸ°<br/>VTA/SNc]
        REW[ë³´ìƒ ì˜ˆì¸¡ ì˜¤ì°¨<br/>Reward Prediction Error]
    end
    
    subgraph ê°•í™”í•™ìŠµ
        TD[TD Error<br/>Î´ = r + Î³V' - V]
    end
    
    DA --> REW
    REW <--> |ë™ì¼?| TD
```

**Schultz et al. (1997)**: ë„íŒŒë¯¼ ë‰´ëŸ°ì´ TD errorë¥¼ ì¸ì½”ë”©

| ìƒí™© | TD Error | ë„íŒŒë¯¼ ë°˜ì‘ |
|------|----------|-------------|
| ì˜ˆìƒì¹˜ ëª»í•œ ë³´ìƒ | Î´ > 0 | ë²„ìŠ¤íŠ¸ ë°œí™” â†‘ |
| ì˜ˆìƒëœ ë³´ìƒ | Î´ â‰ˆ 0 | ë³€í™” ì—†ìŒ |
| ë³´ìƒ ëˆ„ë½ | Î´ < 0 | ë°œí™” ì–µì œ â†“ |

### 2.3 TD í•™ìŠµ êµ¬í˜„

```python
import numpy as np
import matplotlib.pyplot as plt

def td_learning(n_states=5, n_episodes=100, alpha=0.1, gamma=0.9):
    """
    ë‹¨ìˆœ TD(0) í•™ìŠµ
    
    í™˜ê²½: 1D ê²©ì, ì˜¤ë¥¸ìª½ ëì´ ëª©í‘œ (ë³´ìƒ 1)
    """
    V = np.zeros(n_states)
    V_history = [V.copy()]
    td_errors = []
    
    for episode in range(n_episodes):
        s = 0  # ì‹œì‘ ìƒíƒœ
        episode_errors = []
        
        while s < n_states - 1:
            # í–‰ë™: ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™
            s_next = s + 1
            
            # ë³´ìƒ: ëª©í‘œ ë„ë‹¬ ì‹œ 1
            r = 1 if s_next == n_states - 1 else 0
            
            # TD Error
            delta = r + gamma * V[s_next] - V[s]
            episode_errors.append(delta)
            
            # ê°€ì¹˜ ì—…ë°ì´íŠ¸
            V[s] = V[s] + alpha * delta
            
            s = s_next
        
        V_history.append(V.copy())
        td_errors.append(episode_errors)
    
    return V, np.array(V_history), td_errors

# í•™ìŠµ
V, V_history, td_errors = td_learning(n_states=5, n_episodes=50)

# ì‹œê°í™”
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# ê°€ì¹˜ í•¨ìˆ˜ ë³€í™”
for i, ep in enumerate([0, 5, 10, 49]):
    axes[0, 0].plot(V_history[ep], 'o-', label=f'Episode {ep}')
axes[0, 0].set_xlabel('State')
axes[0, 0].set_ylabel('Value V(s)')
axes[0, 0].set_title('Value Function Learning')
axes[0, 0].legend()

# ìµœì¢… ê°€ì¹˜ í•¨ìˆ˜
axes[0, 1].bar(range(5), V, color='steelblue', edgecolor='black')
axes[0, 1].set_xlabel('State')
axes[0, 1].set_ylabel('Value V(s)')
axes[0, 1].set_title('Final Value Function')

# TD Error ë³€í™” (ë„íŒŒë¯¼ ë°˜ì‘ ì‹œë®¬ë ˆì´ì…˜)
early_errors = td_errors[0]  # ì´ˆê¸°
late_errors = td_errors[-1]   # í•™ìŠµ í›„

axes[1, 0].bar(range(len(early_errors)), early_errors, alpha=0.7, label='Early')
axes[1, 0].axhline(y=0, color='gray', linestyle='--')
axes[1, 0].set_xlabel('Step')
axes[1, 0].set_ylabel('TD Error (Î´)')
axes[1, 0].set_title('TD Error = "Dopamine Signal"')
axes[1, 0].legend()

# ë³´ìƒ ì‹œì  ë³€í™”
axes[1, 1].bar(np.arange(len(late_errors)) - 0.2, early_errors, 0.4, 
               alpha=0.7, label='Before learning', color='blue')
axes[1, 1].bar(np.arange(len(late_errors)) + 0.2, late_errors, 0.4,
               alpha=0.7, label='After learning', color='green')
axes[1, 1].axhline(y=0, color='gray', linestyle='--')
axes[1, 1].set_xlabel('Step')
axes[1, 1].set_ylabel('TD Error')
axes[1, 1].set_title('Prediction Error Shift')
axes[1, 1].legend()

plt.tight_layout()
plt.show()
```

---

## 3. Q-ëŸ¬ë‹

### 3.1 Q-í•¨ìˆ˜

$$Q(s, a) = \mathbb{E}[r + \gamma \max_{a'} Q(s', a') | s, a]$$

### 3.2 Q-ëŸ¬ë‹ ì—…ë°ì´íŠ¸

$$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$

### 3.3 êµ¬í˜„ (GridWorld)

```python
class GridWorld:
    def __init__(self, size=4):
        self.size = size
        self.goal = (size-1, size-1)
        self.reset()
    
    def reset(self):
        self.pos = (0, 0)
        return self.pos
    
    def step(self, action):
        # í–‰ë™: 0=ìƒ, 1=í•˜, 2=ì¢Œ, 3=ìš°
        moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]
        new_pos = (
            np.clip(self.pos[0] + moves[action][0], 0, self.size-1),
            np.clip(self.pos[1] + moves[action][1], 0, self.size-1)
        )
        self.pos = new_pos
        
        if self.pos == self.goal:
            return self.pos, 1, True  # ëª©í‘œ ë„ë‹¬
        return self.pos, -0.01, False  # ì‘ì€ íŒ¨ë„í‹°

def q_learning(env, episodes=500, alpha=0.1, gamma=0.95, epsilon=0.1):
    """Q-ëŸ¬ë‹"""
    Q = np.zeros((env.size, env.size, 4))
    rewards_history = []
    
    for episode in range(episodes):
        s = env.reset()
        total_reward = 0
        
        for step in range(100):
            # Îµ-greedy ì •ì±…
            if np.random.rand() < epsilon:
                a = np.random.randint(4)
            else:
                a = np.argmax(Q[s[0], s[1]])
            
            s_next, r, done = env.step(a)
            total_reward += r
            
            # Q ì—…ë°ì´íŠ¸
            Q[s[0], s[1], a] += alpha * (
                r + gamma * np.max(Q[s_next[0], s_next[1]]) - Q[s[0], s[1], a]
            )
            
            s = s_next
            if done:
                break
        
        rewards_history.append(total_reward)
    
    return Q, rewards_history

# Q-ëŸ¬ë‹ ì‹¤í–‰
env = GridWorld(size=4)
Q, rewards = q_learning(env, episodes=300)

# ì‹œê°í™”
fig, axes = plt.subplots(1, 3, figsize=(14, 4))

# í•™ìŠµ ê³¡ì„ 
window = 20
smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')
axes[0].plot(rewards, alpha=0.3, color='blue')
axes[0].plot(range(window-1, len(rewards)), smoothed, 'b-', linewidth=2)
axes[0].set_xlabel('Episode')
axes[0].set_ylabel('Total Reward')
axes[0].set_title('Learning Curve')

# ê°€ì¹˜ í•¨ìˆ˜
V = np.max(Q, axis=2)
im = axes[1].imshow(V, cmap='viridis')
axes[1].set_title('Value Function max Q(s,a)')
plt.colorbar(im, ax=axes[1])
axes[1].set_xlabel('Column')
axes[1].set_ylabel('Row')

# ì •ì±… (í™”ì‚´í‘œ)
policy = np.argmax(Q, axis=2)
arrows = ['â†‘', 'â†“', 'â†', 'â†’']

for i in range(env.size):
    for j in range(env.size):
        if (i, j) != env.goal:
            axes[2].text(j, i, arrows[policy[i, j]], ha='center', va='center', fontsize=16)
        else:
            axes[2].text(j, i, 'â˜…', ha='center', va='center', fontsize=20, color='gold')

axes[2].set_xlim(-0.5, env.size-0.5)
axes[2].set_ylim(env.size-0.5, -0.5)
axes[2].set_title('Learned Policy')
axes[2].grid(True)

plt.tight_layout()
plt.show()
```

---

## 4. Actor-Critic

### 4.1 êµ¬ì¡°

```{mermaid}
flowchart TB
    S[ìƒíƒœ s] --> A[Actor<br/>ì •ì±… Ï€ Î” a|s Î”]
    S --> C[Critic<br/>ê°€ì¹˜ V Î” s Î”]
    
    A --> |í–‰ë™| ENV[í™˜ê²½]
    ENV --> |ë³´ìƒ| C
    C --> |TD Error Î´| A
    
    style A fill:#e74c3c
    style C fill:#3498db
```

### 4.2 ìƒë¬¼í•™ì  í•´ì„

| êµ¬ì„±ìš”ì†Œ | ì—­í•  | ë‡Œ ì˜ì—­ |
|----------|------|---------|
| **Actor** | í–‰ë™ ì„ íƒ | ì„ ì¡°ì²´ (Striatum) |
| **Critic** | ê°€ì¹˜ í‰ê°€ | ë³µì¸¡ ì„ ì¡°ì²´ |
| **TD Error** | í•™ìŠµ ì‹ í˜¸ | ë„íŒŒë¯¼ (VTA) |

---

## 5. ë³´ìƒ ê¸°ë°˜ ì‹œëƒ…ìŠ¤ ê°€ì†Œì„±

### 5.1 3ìš”ì†Œ ê·œì¹™ (Three-Factor Rule)

$$\Delta w = \eta \cdot \underbrace{(\text{pre})}_{\text{ì‹œëƒ…ìŠ¤ì „}} \cdot \underbrace{(\text{post})}_{\text{ì‹œëƒ…ìŠ¤í›„}} \cdot \underbrace{(\text{reward})}_{\text{ì¡°ì ˆì‹ í˜¸}}$$

```python
def three_factor_learning():
    """3ìš”ì†Œ í•™ìŠµ ê·œì¹™ ì‹œë®¬ë ˆì´ì…˜"""
    
    np.random.seed(42)
    n_trials = 100
    n_inputs = 10
    
    # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
    w = np.random.rand(n_inputs) * 0.5
    
    # ì •ë‹µ íŒ¨í„´ (ì¼ë¶€ ì…ë ¥ë§Œ ë³´ìƒê³¼ ì—°ê´€)
    target = np.zeros(n_inputs)
    target[[2, 5, 7]] = 1
    
    w_history = [w.copy()]
    
    for trial in range(n_trials):
        # ëœë¤ ì…ë ¥
        pre = (np.random.rand(n_inputs) > 0.5).astype(float)
        
        # ì¶œë ¥ (ê°€ì¤‘ í•©)
        post = 1 / (1 + np.exp(-np.dot(w, pre) + 2))
        
        # ë³´ìƒ (ì •ë‹µ íŒ¨í„´ê³¼ ìœ ì‚¬í• ìˆ˜ë¡ ë†’ìŒ)
        reward = np.corrcoef(pre, target)[0, 1]
        reward = max(0, reward)  # ì–‘ìˆ˜ë§Œ
        
        # 3ìš”ì†Œ ì—…ë°ì´íŠ¸
        dw = 0.1 * pre * post * reward
        w = w + dw
        w = np.clip(w, 0, 2)
        
        w_history.append(w.copy())
    
    return np.array(w_history), target

w_history, target = three_factor_learning()

# ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# ê°€ì¤‘ì¹˜ ë³€í™”
for i in range(10):
    style = '-' if target[i] == 1 else '--'
    color = 'red' if target[i] == 1 else 'blue'
    axes[0].plot(w_history[:, i], style, color=color, alpha=0.7)

axes[0].set_xlabel('Trial')
axes[0].set_ylabel('Weight')
axes[0].set_title('Weight Evolution\n(Red=target, Blue=non-target)')

# ìµœì¢… ê°€ì¤‘ì¹˜
colors = ['red' if t == 1 else 'blue' for t in target]
axes[1].bar(range(10), w_history[-1], color=colors, edgecolor='black')
axes[1].set_xlabel('Input')
axes[1].set_ylabel('Final Weight')
axes[1].set_title('Learned Weights')

plt.tight_layout()
plt.show()
```

---

## ğŸ“ ì‹¤ìŠµ ë¬¸ì œ

### ë¬¸ì œ 1: SARSA
On-policy ì•Œê³ ë¦¬ì¦˜ì¸ SARSAë¥¼ êµ¬í˜„í•˜ê³  Q-ëŸ¬ë‹ê³¼ ë¹„êµí•˜ì„¸ìš”.

### ë¬¸ì œ 2: ë„íŒŒë¯¼ ì‹œë®¬ë ˆì´ì…˜
ê³ ì „ì  ì¡°ê±´í˜•ì„± ì‹¤í—˜ì˜ ë„íŒŒë¯¼ ë°˜ì‘ì„ TD ëª¨ë¸ë¡œ ì¬í˜„í•˜ì„¸ìš”.

### ë¬¸ì œ 3: íƒìƒ‰-í™œìš©
Îµ-greedy ì™¸ì˜ íƒìƒ‰ ì „ëµ (UCB, Thompson Sampling)ì„ êµ¬í˜„í•˜ì„¸ìš”.

---

## ğŸ”— ê´€ë ¨ ê°œë…

- [Reinforcement Learning](../../concepts/reinforcement-learning)
- [Hebbian Learning](../../concepts/hebbian-learning)
- [STDP](../../concepts/stdp)

---

## ğŸ“š ì°¸ê³  ìë£Œ

- Sutton & Barto, "Reinforcement Learning: An Introduction"
- Schultz et al. (1997): Dopamine neurons
- Dayan & Abbott, Chapter 9

---

## â­ï¸ Next

```{button-ref} ../week8/day1-bci-systems
:color: primary

ë‹¤ìŒ: W8D1 - BCI Systems â†’
```
