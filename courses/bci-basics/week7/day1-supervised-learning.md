---
title: "W7D1 - Supervised Learning"
subtitle: "ì‹ ê²½ê³¼í•™ ê´€ì ì˜ ì§€ë„ í•™ìŠµ"
---

# W7D1: Supervised Learning

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yujinmin2/yujin_research/blob/main/notebooks/W7D1_SupervisedLearning.ipynb)

---

## ğŸ“‹ Overview

**í•µì‹¬ ì§ˆë¬¸**: ë‡ŒëŠ” ì–´ë–»ê²Œ ì…ì¶œë ¥ ë§¤í•‘ì„ í•™ìŠµí•˜ëŠ”ê°€?

**ì§€ë„ í•™ìŠµ**ì€ ì…ë ¥-ì¶œë ¥ ìŒìœ¼ë¡œë¶€í„° í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```{mermaid}
flowchart LR
    X[ì…ë ¥ x] --> NET[ë„¤íŠ¸ì›Œí¬]
    NET --> Y[ì¶œë ¥ Å·]
    T[ì •ë‹µ y] --> ERR[ì˜¤ì°¨ ê³„ì‚°]
    Y --> ERR
    ERR --> |ì—­ì „íŒŒ| NET
```

---

## ğŸ¯ Learning Objectives

1. **í¼ì…‰íŠ¸ë¡ ** í•™ìŠµ ê·œì¹™ ì´í•´
2. **ê²½ì‚¬í•˜ê°•ë²•** êµ¬í˜„
3. **ì—­ì „íŒŒ** ì•Œê³ ë¦¬ì¦˜ ì´í•´
4. **ìƒë¬¼í•™ì  íƒ€ë‹¹ì„±** ë…¼ì˜

---

## 1. í¼ì…‰íŠ¸ë¡  (Perceptron)

### 1.1 ëª¨ë¸

$$y = \sigma\left(\sum_i w_i x_i + b\right) = \sigma(\mathbf{w}^T \mathbf{x} + b)$$

### 1.2 í•™ìŠµ ê·œì¹™

$$\Delta w_i = \eta (y_{target} - y_{pred}) x_i$$

```python
import numpy as np
import matplotlib.pyplot as plt

class Perceptron:
    def __init__(self, n_inputs, lr=0.1):
        self.w = np.random.randn(n_inputs) * 0.1
        self.b = 0
        self.lr = lr
    
    def predict(self, x):
        return 1 if np.dot(self.w, x) + self.b > 0 else 0
    
    def train(self, X, y, epochs=100):
        history = []
        for epoch in range(epochs):
            errors = 0
            for xi, yi in zip(X, y):
                pred = self.predict(xi)
                error = yi - pred
                if error != 0:
                    self.w += self.lr * error * xi
                    self.b += self.lr * error
                    errors += 1
            history.append(errors)
            if errors == 0:
                break
        return history

# AND ê²Œì´íŠ¸ í•™ìŠµ
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_and = np.array([0, 0, 0, 1])
y_or = np.array([0, 1, 1, 1])
y_xor = np.array([0, 1, 1, 0])

fig, axes = plt.subplots(1, 3, figsize=(14, 4))

for ax, (y, name) in zip(axes, [(y_and, 'AND'), (y_or, 'OR'), (y_xor, 'XOR')]):
    p = Perceptron(2)
    history = p.train(X, y, epochs=20)
    
    # ê²°ì • ê²½ê³„
    xx = np.linspace(-0.5, 1.5, 100)
    if p.w[1] != 0:
        yy = -(p.w[0] * xx + p.b) / p.w[1]
        ax.plot(xx, yy, 'g-', linewidth=2, label='Decision boundary')
    
    # ë°ì´í„° í¬ì¸íŠ¸
    ax.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=100, label='Class 0')
    ax.scatter(X[y==1, 0], X[y==1, 1], c='red', s=100, label='Class 1')
    
    accuracy = sum([p.predict(xi) == yi for xi, yi in zip(X, y)]) / len(y)
    ax.set_title(f'{name} Gate\nAccuracy: {accuracy*100:.0f}%')
    ax.set_xlim(-0.5, 1.5)
    ax.set_ylim(-0.5, 1.5)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 2. ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP)

### 2.1 êµ¬ì¡°

```{mermaid}
flowchart LR
    subgraph Input
        X1[xâ‚]
        X2[xâ‚‚]
    end
    
    subgraph Hidden
        H1[hâ‚]
        H2[hâ‚‚]
    end
    
    subgraph Output
        Y[y]
    end
    
    X1 --> H1
    X1 --> H2
    X2 --> H1
    X2 --> H2
    H1 --> Y
    H2 --> Y
```

### 2.2 ìˆœì „íŒŒ (Forward Pass)

$$\mathbf{h} = \sigma(\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)})$$
$$\mathbf{y} = \sigma(\mathbf{W}^{(2)} \mathbf{h} + \mathbf{b}^{(2)})$$

### 2.3 êµ¬í˜„

```python
class MLP:
    def __init__(self, layer_sizes, lr=0.5):
        self.layers = []
        self.lr = lr
        
        for i in range(len(layer_sizes) - 1):
            W = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * 0.5
            b = np.zeros(layer_sizes[i+1])
            self.layers.append({'W': W, 'b': b})
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def sigmoid_deriv(self, x):
        return x * (1 - x)
    
    def forward(self, x):
        activations = [x]
        for layer in self.layers:
            z = layer['W'] @ activations[-1] + layer['b']
            a = self.sigmoid(z)
            activations.append(a)
        return activations
    
    def backward(self, activations, y_true):
        deltas = [None] * len(self.layers)
        
        # ì¶œë ¥ì¸µ ì˜¤ì°¨
        output_error = y_true - activations[-1]
        deltas[-1] = output_error * self.sigmoid_deriv(activations[-1])
        
        # ì—­ì „íŒŒ
        for i in range(len(self.layers) - 2, -1, -1):
            error = self.layers[i+1]['W'].T @ deltas[i+1]
            deltas[i] = error * self.sigmoid_deriv(activations[i+1])
        
        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        for i in range(len(self.layers)):
            self.layers[i]['W'] += self.lr * np.outer(deltas[i], activations[i])
            self.layers[i]['b'] += self.lr * deltas[i]
    
    def train(self, X, y, epochs=1000):
        losses = []
        for epoch in range(epochs):
            total_loss = 0
            for xi, yi in zip(X, y):
                activations = self.forward(xi)
                loss = 0.5 * np.sum((yi - activations[-1])**2)
                total_loss += loss
                self.backward(activations, yi)
            losses.append(total_loss / len(X))
        return losses
    
    def predict(self, x):
        return self.forward(x)[-1]

# XOR ë¬¸ì œ í•´ê²°
X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_xor = np.array([[0], [1], [1], [0]])

mlp = MLP([2, 4, 1], lr=1.0)
losses = mlp.train(X_xor, y_xor, epochs=5000)

# ê²°ê³¼
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# í•™ìŠµ ê³¡ì„ 
axes[0].plot(losses, 'b-')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Learning Curve')
axes[0].set_yscale('log')

# ê²°ì • ê²½ê³„
xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 50), np.linspace(-0.5, 1.5, 50))
Z = np.array([mlp.predict(np.array([x, y]))[0] for x, y in zip(xx.ravel(), yy.ravel())])
Z = Z.reshape(xx.shape)

axes[1].contourf(xx, yy, Z, levels=20, cmap='RdBu_r', alpha=0.7)
axes[1].scatter(X_xor[y_xor.flatten()==0, 0], X_xor[y_xor.flatten()==0, 1], 
                c='blue', s=100, edgecolors='black')
axes[1].scatter(X_xor[y_xor.flatten()==1, 0], X_xor[y_xor.flatten()==1, 1], 
                c='red', s=100, edgecolors='black')
axes[1].set_title('XOR Decision Boundary')
axes[1].set_xlabel('xâ‚')
axes[1].set_ylabel('xâ‚‚')

plt.tight_layout()
plt.show()

# ì˜ˆì¸¡ ê²°ê³¼
print("XOR ì˜ˆì¸¡ ê²°ê³¼:")
for xi, yi in zip(X_xor, y_xor):
    pred = mlp.predict(xi)[0]
    print(f"  {xi} â†’ {pred:.3f} (ì •ë‹µ: {yi[0]})")
```

---

## 3. ì—­ì „íŒŒì˜ ìƒë¬¼í•™ì  íƒ€ë‹¹ì„±

### 3.1 ë¬¸ì œì 

| ë¬¸ì œ | ì„¤ëª… |
|------|------|
| **ê°€ì¤‘ì¹˜ ìˆ˜ì†¡** | ì—­ì „íŒŒ ì‹œ ë™ì¼í•œ W ì‚¬ìš© |
| **ë¯¸ë¶„ í•„ìš”** | ë‰´ëŸ°ì´ ë¯¸ë¶„ ê³„ì‚°? |
| **ì‹œê°„ì  ë¹„êµ­ì†Œì„±** | ìˆœì „íŒŒ ì™„ë£Œ í›„ ì—­ì „íŒŒ |
| **ì–‘ë°©í–¥ ì‹œëƒ…ìŠ¤** | í•œ ì‹œëƒ…ìŠ¤ë¡œ ì–‘ë°©í–¥ ì „ë‹¬ |

### 3.2 ìƒë¬¼í•™ì  ëŒ€ì•ˆ

```{mermaid}
flowchart TB
    subgraph ì—­ì „íŒŒ
        BP[í‘œì¤€ ì—­ì „íŒŒ<br/>Weight Transport]
    end
    
    subgraph ëŒ€ì•ˆ
        FA[Feedback Alignment<br/>ëœë¤ ì—­ì „íŒŒ]
        PREDICT[Predictive Coding<br/>ì˜ˆì¸¡ ì˜¤ì°¨]
        EQUIV[Equilibrium Propagation<br/>í‰í˜• ì „íŒŒ]
        LOCAL[Local Learning<br/>êµ­ì†Œ í•™ìŠµ]
    end
    
    BP --> FA
    BP --> PREDICT
    BP --> EQUIV
    BP --> LOCAL
```

---

## 4. Delta Ruleê³¼ LMS

### 4.1 Delta Rule

$$\Delta w_{ij} = \eta \cdot (y_j^{target} - y_j) \cdot x_i$$

### 4.2 ìƒë¬¼í•™ì  í•´ì„

| ìš”ì†Œ | ìˆ˜í•™ì  | ìƒë¬¼í•™ì  |
|------|--------|----------|
| $\eta$ | í•™ìŠµë¥  | ê°€ì†Œì„± ì¡°ì ˆ |
| $y^{target} - y$ | ì˜¤ì°¨ | ë„íŒŒë¯¼ ì‹ í˜¸? |
| $x_i$ | ì…ë ¥ | ì‹œëƒ…ìŠ¤ì „ í™œë™ |
| $y_j$ | ì¶œë ¥ | ì‹œëƒ…ìŠ¤í›„ í™œë™ |

---

## ğŸ“ ì‹¤ìŠµ ë¬¸ì œ

### ë¬¸ì œ 1: MNIST ë¶„ë¥˜
MLPë¡œ MNIST ì†ê¸€ì”¨ ìˆ«ì ë¶„ë¥˜ê¸°ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.

### ë¬¸ì œ 2: Feedback Alignment
ëœë¤ ì—­ë°©í–¥ ê°€ì¤‘ì¹˜ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•œì§€ í™•ì¸í•˜ì„¸ìš”.

### ë¬¸ì œ 3: ì˜¨ë¼ì¸ í•™ìŠµ
ë¯¸ë‹ˆë°°ì¹˜ ëŒ€ì‹  ë‹¨ì¼ ìƒ˜í”Œë¡œ í•™ìŠµí•˜ëŠ” ì˜¨ë¼ì¸ ë²„ì „ì„ êµ¬í˜„í•˜ì„¸ìš”.

---

## ğŸ”— ê´€ë ¨ ê°œë…

- [Supervised Learning](../../concepts/supervised-learning)
- [Hebbian Learning](../../concepts/hebbian-learning)
- [STDP](../../concepts/stdp)

---

## ğŸ“š ì°¸ê³  ìë£Œ

- Dayan & Abbott, Chapter 10
- Lillicrap et al., "Backpropagation and the brain"
- Rumelhart et al. (1986): ì—­ì „íŒŒ ì›ë…¼ë¬¸

---

## â­ï¸ Next

```{button-ref} day2-reinforcement-learning
:color: primary

ë‹¤ìŒ: W7D2 - Reinforcement Learning â†’
```
