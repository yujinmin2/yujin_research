---
title: "W6D1 - Synaptic Models"
subtitle: "ì‹œëƒ…ìŠ¤ ì „ë‹¬ê³¼ ê°€ì†Œì„± ëª¨ë¸ë§"
---

# W6D1: Synaptic Models

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yujinmin2/yujin_research/blob/main/notebooks/W6D1_SynapticModels.ipynb)

---

## ğŸ“‹ Overview

**í•µì‹¬ ì§ˆë¬¸**: ë‰´ëŸ° ê°„ì˜ ì‹œëƒ…ìŠ¤ ì—°ê²°ì„ ì–´ë–»ê²Œ ëª¨ë¸ë§í•˜ëŠ”ê°€?

ì‹œëƒ…ìŠ¤ëŠ” ë‰´ëŸ° ê°„ ì •ë³´ ì „ë‹¬ì˜ í•µì‹¬ì´ë©°, **ê°€ì†Œì„±**ì„ í†µí•´ í•™ìŠµì´ ì¼ì–´ë‚©ë‹ˆë‹¤.

```{mermaid}
flowchart LR
    PRE[ì‹œëƒ…ìŠ¤ì „ ë‰´ëŸ°<br/>Spike!] --> |ì‹ ê²½ì „ë‹¬ë¬¼ì§ˆ| SYN[ì‹œëƒ…ìŠ¤]
    SYN --> |ì „ë¥˜ ì£¼ì…| POST[ì‹œëƒ…ìŠ¤í›„ ë‰´ëŸ°<br/>EPSP/IPSP]
    
    style PRE fill:#e74c3c
    style SYN fill:#f39c12
    style POST fill:#3498db
```

---

## ğŸ¯ Learning Objectives

1. **í™”í•™ ì‹œëƒ…ìŠ¤**ì˜ ê¸°ë³¸ ëª¨ë¸ ì´í•´
2. **EPSP/IPSP** ì‹œë®¬ë ˆì´ì…˜
3. **ë‹¨ê¸° ê°€ì†Œì„±** (STD/STF) êµ¬í˜„
4. **STDP** (ì¥ê¸° ê°€ì†Œì„±) êµ¬í˜„

---

## 1. ì‹œëƒ…ìŠ¤ ì „ë¥˜ ëª¨ë¸

### 1.1 ë¸íƒ€ ì‹œëƒ…ìŠ¤ (ê°€ì¥ ë‹¨ìˆœ)

ìŠ¤íŒŒì´í¬ ì‹œ ì¦‰ê°ì ì¸ ì „ë¥˜ ì£¼ì…:

$$I_{syn}(t) = w \cdot \sum_k \delta(t - t_k)$$

### 1.2 ì§€ìˆ˜ ì‹œëƒ…ìŠ¤

ë” í˜„ì‹¤ì : ì‹œê°„ì— ë”°ë¼ ê°ì‡ 

$$\tau_s \frac{dI_{syn}}{dt} = -I_{syn}$$

ìŠ¤íŒŒì´í¬ ì‹œ: $I_{syn} \leftarrow I_{syn} + w$

### 1.3 ì´ì¤‘ ì§€ìˆ˜ ì‹œëƒ…ìŠ¤

ìƒìŠ¹ê³¼ í•˜ê°• ì‹œê°„ ë¶„ë¦¬:

$$I_{syn}(t) = \bar{g} \cdot \frac{\tau_d}{\tau_d - \tau_r} \left( e^{-t/\tau_d} - e^{-t/\tau_r} \right)$$

```python
import numpy as np
import matplotlib.pyplot as plt

def alpha_synapse(t, tau=5):
    """ì•ŒíŒŒ í•¨ìˆ˜ ì‹œëƒ…ìŠ¤"""
    return (t / tau) * np.exp(1 - t / tau) * (t >= 0)

def double_exp_synapse(t, tau_r=1, tau_d=5):
    """ì´ì¤‘ ì§€ìˆ˜ ì‹œëƒ…ìŠ¤"""
    if tau_d == tau_r:
        tau_d = tau_r + 0.1
    norm = tau_d / (tau_d - tau_r)
    return norm * (np.exp(-t / tau_d) - np.exp(-t / tau_r)) * (t >= 0)

# ì‹œê°í™”
t = np.linspace(0, 50, 500)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# ë‹¤ì–‘í•œ ì‹œëƒ…ìŠ¤ ì»¤ë„
axes[0].plot(t, alpha_synapse(t, tau=2), label='Ï„=2ms (fast)')
axes[0].plot(t, alpha_synapse(t, tau=5), label='Ï„=5ms')
axes[0].plot(t, alpha_synapse(t, tau=10), label='Ï„=10ms (slow)')
axes[0].set_xlabel('Time (ms)')
axes[0].set_ylabel('Synaptic Current')
axes[0].set_title('Alpha Synapse')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# AMPA vs NMDA vs GABA
axes[1].plot(t, double_exp_synapse(t, 0.5, 3), 'b-', label='AMPA (fast)')
axes[1].plot(t, double_exp_synapse(t, 5, 100), 'g-', label='NMDA (slow)')
axes[1].plot(t, -double_exp_synapse(t, 1, 7), 'r-', label='GABA (inhibitory)')
axes[1].set_xlabel('Time (ms)')
axes[1].set_ylabel('Synaptic Current')
axes[1].set_title('Different Synapse Types')
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].axhline(y=0, color='gray', linestyle='--')

plt.tight_layout()
plt.show()
```

---

## 2. ì‹œëƒ…ìŠ¤ ì „ë‹¬ ì‹œë®¬ë ˆì´ì…˜

### 2.1 ì‹œëƒ…ìŠ¤í›„ ì „ìœ„ (PSP)

```python
def simulate_synapse_lif(spike_times, w=5, tau_s=5, tau_m=20, 
                          E_L=-70, V_th=-55, duration=100, dt=0.1):
    """
    ì‹œëƒ…ìŠ¤ ì…ë ¥ì„ ë°›ëŠ” LIF ë‰´ëŸ°
    """
    t = np.arange(0, duration, dt)
    V = np.zeros(len(t))
    I_syn = np.zeros(len(t))
    V[0] = E_L
    
    output_spikes = []
    
    for i in range(1, len(t)):
        # ì‹œëƒ…ìŠ¤ ì „ë¥˜ ê°ì‡ 
        dI = -I_syn[i-1] / tau_s * dt
        I_syn[i] = I_syn[i-1] + dI
        
        # ì…ë ¥ ìŠ¤íŒŒì´í¬ ì²´í¬
        if any(abs(t[i] - st) < dt for st in spike_times):
            I_syn[i] += w
        
        # LIF ë™ì—­í•™
        dV = (-(V[i-1] - E_L) + I_syn[i]) / tau_m * dt
        V[i] = V[i-1] + dV
        
        if V[i] >= V_th:
            output_spikes.append(t[i])
            V[i] = E_L
    
    return t, V, I_syn, output_spikes

# ë‹¨ì¼ EPSP
spike_times = [20]
t, V, I_syn, _ = simulate_synapse_lif(spike_times, w=10, duration=80)

fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)

# ì…ë ¥ ìŠ¤íŒŒì´í¬
axes[0].eventplot(spike_times, linewidths=2)
axes[0].set_ylabel('Pre-synaptic')
axes[0].set_title('Single Synaptic Input â†’ EPSP')

# ì‹œëƒ…ìŠ¤ ì „ë¥˜
axes[1].plot(t, I_syn, 'g-', linewidth=2)
axes[1].set_ylabel('I_syn')

# ë§‰ì „ìœ„
axes[2].plot(t, V, 'b-', linewidth=2)
axes[2].axhline(y=-55, color='red', linestyle='--', label='Threshold')
axes[2].set_xlabel('Time (ms)')
axes[2].set_ylabel('V (mV)')
axes[2].legend()

plt.tight_layout()
plt.show()
```

### 2.2 ì‹œê°„ì  í•©ì‚° (Temporal Summation)

```python
# ë¹ˆë„ì— ë”°ë¥¸ í•©ì‚°
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

frequencies = [20, 50, 100, 200]  # Hz

for ax, freq in zip(axes.flat, frequencies):
    isi = 1000 / freq  # ms
    spike_times = np.arange(10, 90, isi)
    
    t, V, I_syn, output = simulate_synapse_lif(spike_times, w=3, duration=100)
    
    ax.plot(t, V, 'b-', linewidth=1.5)
    ax.axhline(y=-55, color='red', linestyle='--', alpha=0.5)
    ax.eventplot(spike_times, lineoffsets=-80, linelengths=5, colors='green')
    ax.set_xlabel('Time (ms)')
    ax.set_ylabel('V (mV)')
    ax.set_title(f'{freq} Hz input â†’ {len(output)} output spikes')
    ax.set_ylim(-85, -50)

plt.suptitle('Temporal Summation', fontsize=14)
plt.tight_layout()
plt.show()
```

---

## 3. ë‹¨ê¸° ê°€ì†Œì„± (Short-Term Plasticity)

### 3.1 STD (Short-Term Depression)

ë°˜ë³µ ìê·¹ ì‹œ ì‹œëƒ…ìŠ¤ ê°•ë„ **ê°ì†Œ**

$$\frac{dx}{dt} = \frac{1-x}{\tau_d} - u \cdot x \cdot \delta(t - t_{spike})$$

### 3.2 STF (Short-Term Facilitation)

ë°˜ë³µ ìê·¹ ì‹œ ì‹œëƒ…ìŠ¤ ê°•ë„ **ì¦ê°€**

$$\frac{du}{dt} = \frac{U-u}{\tau_f} + U(1-u) \cdot \delta(t - t_{spike})$$

### 3.3 êµ¬í˜„ (Tsodyks-Markram Model)

```python
def simulate_stp(spike_times, U=0.2, tau_d=200, tau_f=50, 
                 A=1.0, duration=500, dt=0.1):
    """
    Short-Term Plasticity (Tsodyks-Markram ëª¨ë¸)
    
    U : ê¸°ë³¸ ë°©ì¶œ í™•ë¥ 
    tau_d : depression ì‹œê°„ìƒìˆ˜
    tau_f : facilitation ì‹œê°„ìƒìˆ˜
    """
    t = np.arange(0, duration, dt)
    
    x = np.ones(len(t))  # ê°€ìš© ìì› (depression)
    u = np.ones(len(t)) * U  # ë°©ì¶œ í™•ë¥  (facilitation)
    PSP = np.zeros(len(t))
    
    for i in range(1, len(t)):
        # íšŒë³µ ë™ì—­í•™
        dx = (1 - x[i-1]) / tau_d * dt
        du = (U - u[i-1]) / tau_f * dt
        
        x[i] = x[i-1] + dx
        u[i] = u[i-1] + du
        
        # ìŠ¤íŒŒì´í¬ ì²´í¬
        if any(abs(t[i] - st) < dt for st in spike_times):
            PSP[i] = A * u[i] * x[i]
            
            # ì—…ë°ì´íŠ¸ (ìŠ¤íŒŒì´í¬ í›„)
            x[i] = x[i] * (1 - u[i])
            u[i] = u[i] + U * (1 - u[i])
    
    return t, x, u, PSP

# STD vs STF ë¹„êµ
fig, axes = plt.subplots(3, 2, figsize=(14, 10))

spike_times = np.arange(50, 450, 50)  # 20Hz

# STD dominant
t, x_d, u_d, psp_d = simulate_stp(spike_times, U=0.5, tau_d=200, tau_f=20)

# STF dominant  
t, x_f, u_f, psp_f = simulate_stp(spike_times, U=0.1, tau_d=50, tau_f=200)

# ì™¼ìª½: STD
axes[0, 0].eventplot(spike_times, linewidths=1.5)
axes[0, 0].set_title('STD Dominant (U=0.5, Ï„d=200, Ï„f=20)')
axes[0, 0].set_ylabel('Spikes')

axes[1, 0].plot(t, x_d, 'b-', label='x (resources)')
axes[1, 0].plot(t, u_d, 'r-', label='u (release prob)')
axes[1, 0].legend()
axes[1, 0].set_ylabel('Variables')

axes[2, 0].stem(spike_times, [psp_d[int(st/0.1)] for st in spike_times], 
                basefmt=' ', linefmt='g-', markerfmt='go')
axes[2, 0].set_xlabel('Time (ms)')
axes[2, 0].set_ylabel('PSP')

# ì˜¤ë¥¸ìª½: STF
axes[0, 1].eventplot(spike_times, linewidths=1.5)
axes[0, 1].set_title('STF Dominant (U=0.1, Ï„d=50, Ï„f=200)')

axes[1, 1].plot(t, x_f, 'b-', label='x (resources)')
axes[1, 1].plot(t, u_f, 'r-', label='u (release prob)')
axes[1, 1].legend()

axes[2, 1].stem(spike_times, [psp_f[int(st/0.1)] for st in spike_times],
                basefmt=' ', linefmt='g-', markerfmt='go')
axes[2, 1].set_xlabel('Time (ms)')

plt.tight_layout()
plt.show()
```

---

## 4. STDP (Spike-Timing-Dependent Plasticity)

### 4.1 ê°œë…

```{mermaid}
flowchart LR
    subgraph LTP["Pre â†’ Post (LTP)"]
        PRE1[Pre spike] --> |Î”t > 0| POST1[Post spike]
        POST1 --> W_UP[ê°€ì¤‘ì¹˜ â†‘]
    end
    
    subgraph LTD["Post â†’ Pre (LTD)"]
        POST2[Post spike] --> |Î”t < 0| PRE2[Pre spike]
        PRE2 --> W_DN[ê°€ì¤‘ì¹˜ â†“]
    end
```

### 4.2 STDP ê·œì¹™

$$\Delta w = \begin{cases} 
A_+ e^{-\Delta t / \tau_+} & \text{if } \Delta t > 0 \text{ (LTP)} \\
-A_- e^{\Delta t / \tau_-} & \text{if } \Delta t < 0 \text{ (LTD)}
\end{cases}$$

### 4.3 êµ¬í˜„

```python
def stdp_window(dt, A_plus=0.01, A_minus=0.012, tau_plus=20, tau_minus=20):
    """STDP í•™ìŠµ ì°½"""
    if dt > 0:
        return A_plus * np.exp(-dt / tau_plus)
    else:
        return -A_minus * np.exp(dt / tau_minus)

# STDP ì°½ ì‹œê°í™”
dt_range = np.linspace(-50, 50, 200)
dw = [stdp_window(dt) for dt in dt_range]

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

axes[0].plot(dt_range, dw, 'b-', linewidth=2)
axes[0].axhline(y=0, color='gray', linestyle='--')
axes[0].axvline(x=0, color='gray', linestyle='--')
axes[0].fill_between(dt_range, dw, where=np.array(dw)>0, alpha=0.3, color='green', label='LTP')
axes[0].fill_between(dt_range, dw, where=np.array(dw)<0, alpha=0.3, color='red', label='LTD')
axes[0].set_xlabel('Î”t = t_post - t_pre (ms)')
axes[0].set_ylabel('Î”w')
axes[0].set_title('STDP Learning Window')
axes[0].legend()

# STDP ì‹œë®¬ë ˆì´ì…˜
def simulate_stdp_learning(pre_rate=20, post_rate=20, correlation=0.5, 
                           duration=1000, w_init=0.5):
    """ìƒê´€ê´€ê³„ì— ë”°ë¥¸ STDP í•™ìŠµ"""
    dt = 0.1
    t = np.arange(0, duration, dt)
    
    # ìŠ¤íŒŒì´í¬ ìƒì„±
    np.random.seed(42)
    pre_spikes = np.random.rand(len(t)) < (pre_rate / 1000 * dt)
    
    # ìƒê´€ê´€ê³„ ìˆëŠ” post ìŠ¤íŒŒì´í¬
    post_spikes = np.zeros(len(t), dtype=bool)
    delay = int(10 / dt)  # 10ms delay
    
    for i in range(delay, len(t)):
        if pre_spikes[i - delay]:
            if np.random.rand() < correlation:
                post_spikes[i] = True
        if np.random.rand() < ((1-correlation) * post_rate / 1000 * dt):
            post_spikes[i] = True
    
    # STDP í•™ìŠµ
    w = w_init
    w_history = [w]
    
    pre_times = t[pre_spikes]
    post_times = t[post_spikes]
    
    for t_post in post_times:
        for t_pre in pre_times:
            dt_spike = t_post - t_pre
            if abs(dt_spike) < 50:
                w += stdp_window(dt_spike)
                w = np.clip(w, 0, 1)
        w_history.append(w)
    
    return w_history

# ë‹¤ì–‘í•œ ìƒê´€ê´€ê³„
correlations = [0.0, 0.3, 0.6, 0.9]
for corr in correlations:
    w_hist = simulate_stdp_learning(correlation=corr)
    axes[1].plot(w_hist[:100], label=f'corr={corr}')

axes[1].set_xlabel('Post-synaptic spikes')
axes[1].set_ylabel('Synaptic Weight')
axes[1].set_title('STDP Learning vs Correlation')
axes[1].legend()
axes[1].set_ylim(0, 1)

plt.tight_layout()
plt.show()
```

---

## ğŸ“ ì‹¤ìŠµ ë¬¸ì œ

### ë¬¸ì œ 1: NMDA ì „ì•• ì˜ì¡´ì„±
MgÂ²âº ë¸”ë¡ì„ í¬í•¨í•œ NMDA ì‹œëƒ…ìŠ¤ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.

### ë¬¸ì œ 2: STP í•„í„°ë§
STD ì‹œëƒ…ìŠ¤ê°€ ì €ì£¼íŒŒ í†µê³¼ í•„í„° ì—­í• ì„ í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

### ë¬¸ì œ 3: STDP íŒ¨í„´ í•™ìŠµ
STDPë¡œ ì…ë ¥ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë‰´ëŸ°ì„ êµ¬í˜„í•˜ì„¸ìš”.

---

## ğŸ”— ê´€ë ¨ ê°œë…

- [ì‹œëƒ…ìŠ¤](../../concepts/synapse)
- [STDP](../../concepts/stdp)
- [Hebbian Learning](../../concepts/hebbian-learning)

---

## ğŸ“š ì°¸ê³  ìë£Œ

- Dayan & Abbott, Chapter 5: Synaptic Conductance
- Gerstner & Kistler, Chapter 11: Synaptic Plasticity
- Bi & Poo (1998): STDP ì›ë…¼ë¬¸

---

## â­ï¸ Next

```{button-ref} day2-network-models
:color: primary

ë‹¤ìŒ: W6D2 - Network Models â†’
```
